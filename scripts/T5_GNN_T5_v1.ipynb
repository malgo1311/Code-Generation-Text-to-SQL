{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # MOUNTING GOOGLE DRIVE\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import os\n",
    "# print(os.getcwd())\n",
    "\n",
    "# wd = '/content/drive/MyDrive/CS 685/cs685_project/notebooks'\n",
    "# print(os.listdir(wd))\n",
    "# os.chdir(wd)\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atharva\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.1.0)/charset_normalizer (2.0.10) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "C:\\Users\\Atharva\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install tokenizers\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "from load_dataset import Text2SQLDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tokenizers import AddedToken\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers import BertModel, T5ForConditionalGeneration, T5Tokenizer, BertTokenizer\n",
    "\n",
    "from graph import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T04:26:41.509135300Z",
     "start_time": "2023-05-12T04:26:15.837127300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# FOR PRINTING INTERMEDIATE TORCH SIZES\n",
    "DEBUG_FLAG = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T04:26:41.524349700Z",
     "start_time": "2023-05-12T04:26:41.512460300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "# class GNN(torch.nn.Module):\n",
    "#     # single layer!!\n",
    "#     def __init__(self, in_feats, hidden_feats, out_feats):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = dgl.nn.GraphConv(in_feats, hidden_feats)\n",
    "#         self.conv2 = dgl.nn.GraphConv(hidden_feats, out_feats)\n",
    "#\n",
    "#     def forward(self, g, h):\n",
    "#         h = torch.relu(self.conv1(g, h))\n",
    "#         h = self.conv2(g, h)\n",
    "#         return h\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, bert_hidden_size, t5_hidden_size, max_input_length,\n",
    "                 max_output_length, bert_model, t5_model, batch_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "        self.t5_hidden_size = t5_hidden_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(t5_model)\n",
    "#         self.linear = nn.Linear(bert_hidden_size, t5_hidden_size)\n",
    "        self.linear = nn.Linear(bert_hidden_size, max_input_length*t5_hidden_size)\n",
    "\n",
    "        self.gnn = GNNModel(max_input_length, t5_hidden_size, max_input_length)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, input_mask,\n",
    "                decoder_input_ids, decoder_attention_mask):\n",
    "\n",
    "        # Encode input with BERT\n",
    "        _, bert_output = self.bert(input_ids=input_ids, attention_mask=input_mask, return_dict=False)\n",
    "\n",
    "        if DEBUG_FLAG: print(f\"bert_output - {bert_output.size()}\")\n",
    "\n",
    "        # Transform BERT output to T5 input shape\n",
    "        t5_input = self.linear(bert_output)\n",
    "        graph_g = create_graph(t5_input)\n",
    "        if DEBUG_FLAG: print(f\"bert_output linear - {t5_input.size()}\")\n",
    "\n",
    "#         t5_input = t5_input.unsqueeze(1).repeat(1, self.max_output_length, 1)\n",
    "#         if DEBUG_FLAG: print(f\"bert_output linear unsqueeze - {t5_input.size()}\")\n",
    "\n",
    "        t5_input = t5_input.view(self.batch_size, self.max_input_length, self.t5_hidden_size)\n",
    "        t5_input = t5_input.unsqueeze(0)\n",
    "        if DEBUG_FLAG: print(f\"t5_input - {t5_input.size()}\")\n",
    "\n",
    "#         t5_outputs = self.t5(decoder_input_ids=decoder_input_ids,\n",
    "#                              decoder_attention_mask=decoder_attention_mask,\n",
    "#                              encoder_outputs=t5_input\n",
    "#                             )\n",
    "#         if DEBUG_FLAG: print(f\"t5_input logits - {(t5_outputs.logits).size()}\")\n",
    "#         return t5_outputs.logits\n",
    "\n",
    "        t5_outputs = self.t5(labels=decoder_input_ids,\n",
    "                             decoder_attention_mask=decoder_attention_mask,\n",
    "                             encoder_outputs=t5_input,\n",
    "                             return_dict = True\n",
    "                            )\n",
    "        # add gnn embeddigns to t5_output\n",
    "        gnn_enc = self.gnn(graph_g,t5_outputs) + t5_outputs\n",
    "\n",
    "        # give gnn + t5 encodings (Graphix layers) to final decoder\n",
    "\n",
    "        final_outputs = self.t5(labels=decoder_input_ids,\n",
    "                             decoder_attention_mask=decoder_attention_mask,\n",
    "                             encoder_outputs=gnn_enc,\n",
    "                             return_dict = True\n",
    "                            )\n",
    "\n",
    "        if DEBUG_FLAG: print(f\"t5_input - {type(final_outputs)}\")\n",
    "        return gnn_enc\n",
    "\n",
    "    def predict(self, input_ids, input_mask, batch_size, t5_tokenizer):\n",
    "\n",
    "        _, bert_output = self.bert(input_ids=input_ids, attention_mask=input_mask, return_dict=False)\n",
    "        if DEBUG_FLAG: print(f\"bert_output - {bert_output.size()}\")\n",
    "\n",
    "        # Transform BERT output to T5 input shape\n",
    "        t5_input = self.linear(bert_output)\n",
    "        if DEBUG_FLAG: print(f\"t5_input - {t5_input.size()} - {t5_input}\")\n",
    "        t5_input = t5_input.view(batch_size, self.max_input_length, self.t5_hidden_size)\n",
    "        t5_input = t5_input.unsqueeze(0)\n",
    "        if DEBUG_FLAG: print(f\"t5_input - {t5_input.size()} - {t5_input}\")\n",
    "\n",
    "        t5_input = t5_input.to(torch.LongTensor) #int64)\n",
    "\n",
    "#         t5_outputs = self.t5.generate(t5_input, max_length = 127)\n",
    "\n",
    "        #######################################\n",
    "\n",
    "#         # Generate initial input for T5 decoder\n",
    "        start_token = t5_tokenizer.pad_token_id\n",
    "\n",
    "#         decoder_input_ids = torch.tensor([start_token] * batch_size).unsqueeze(0)\n",
    "#         decoder_attention_mask = torch.tensor([1] * batch_size).unsqueeze(0)\n",
    "\n",
    "        decoder_input_ids = torch.tensor([start_token]).unsqueeze(0)\n",
    "        decoder_attention_mask = torch.tensor([1]).unsqueeze(0)\n",
    "\n",
    "#         decoder_input_ids = decoder_input_ids.view(decoder_input_ids.shape[1],\n",
    "#                                                    decoder_input_ids.shape[0])\n",
    "#         decoder_attention_mask = decoder_attention_mask.view(decoder_attention_mask.shape[1],\n",
    "#                                                              decoder_attention_mask.shape[0])\n",
    "\n",
    "        print(f\"decoder_input_ids - {decoder_input_ids.size()}\")\n",
    "        print(f\"decoder_attention_mask - {decoder_attention_mask.size()}\")\n",
    "\n",
    "        print(f\"initial decoder_input_ids - {decoder_input_ids}\")\n",
    "        # Use the model to get output logits\n",
    "        # Predict the output\n",
    "        with torch.no_grad():\n",
    "            for i in range(50):  # Maximum length of generated sequence\n",
    "                t5_outputs = self.t5(decoder_input_ids=decoder_input_ids,\n",
    "                                     decoder_attention_mask=decoder_attention_mask,\n",
    "                                     encoder_outputs=t5_input)\n",
    "#                 print(f\"t5_outputs - {t5_outputs}\")\n",
    "                print(f\"t5_outputs logits - {(t5_outputs.logits).size()}\")\n",
    "\n",
    "                next_token_logits = t5_outputs.logits[:, -1, :]\n",
    "                print(f\"next_token_logits - {next_token_logits.size()}\")\n",
    "\n",
    "#                 next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "                next_token_id = next_token_logits.argmax(1)\n",
    "#                 print(f\"next_token_id - {next_token_id.size()}\")\n",
    "#                 print(f\"next_token_id.unsqueeze(-1) - {next_token_id.unsqueeze(-1).size()}\")\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
    "                decoder_attention_mask = torch.cat([decoder_attention_mask,\n",
    "                                                    torch.ones_like(next_token_id.unsqueeze(-1))], dim=-1)\n",
    "\n",
    "                if next_token_id == t5_tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "                print(f\"pred decoder_input_ids - {decoder_input_ids}\")\n",
    "\n",
    "#                 break\n",
    "\n",
    "        # generated_text\n",
    "        t5_outputs = t5_tokenizer.decode(decoder_input_ids.squeeze(), skip_special_tokens=True)\n",
    "        #######################################\n",
    "\n",
    "        return t5_outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T04:28:48.612626900Z",
     "start_time": "2023-05-12T04:28:48.596632900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# must remain same!!\n",
    "def train(train_filepath, batch_size, bert_hidden_size, t5_hidden_size, lr, num_epochs,\n",
    "         max_input_length, max_output_length, bert_model, t5_model):\n",
    "\n",
    "    sub_folder_name = f\"BERT_T5_lr{lr}_bs{batch_size}_{bert_model}_{t5_model}\"\n",
    "    models_directory = f\"models/{sub_folder_name}\"\n",
    "\n",
    "    if not os.path.isdir(models_directory):\n",
    "        os.makedirs(models_directory)\n",
    "\n",
    "    # TENSORBOARD\n",
    "    writer = SummaryWriter(f'tb/loss_plot/{sub_folder_name}')\n",
    "\n",
    "    train_dataset = Text2SQLDataset(\n",
    "            dir_ = train_filepath,\n",
    "            mode = \"train\")\n",
    "\n",
    "    train_dataloder = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = True,\n",
    "            collate_fn = lambda x: x,\n",
    "            drop_last = True\n",
    "        )\n",
    "\n",
    "    print(f\"Number of batches - {len(train_dataloder)}\")\n",
    "\n",
    "    # Define BERT and T5 tokenizers\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
    "    print(f\"Tokenizers loaded\")\n",
    "\n",
    "    model = EncoderDecoder(bert_hidden_size, t5_hidden_size,\n",
    "                           max_input_length, max_output_length,\n",
    "                           bert_model, t5_model, batch_size).to(device)\n",
    "    print(f\"Model loaded\")\n",
    "#     print(f\"{model.config.decoder_start_token_id}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    print(f\"Otimizer - Adam\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=t5_tokenizer.pad_token_id)\n",
    "    print(f\"CrossEntropyLoss initialized\")\n",
    "\n",
    "    # initialize array of losses\n",
    "    losses = {'train': {}, \"val\": {}}\n",
    "\n",
    "    # for epoch in range(num_epochs):\n",
    "    with trange(num_epochs) as tr:\n",
    "        for epoch in tr:\n",
    "\n",
    "            # Train the model\n",
    "            model.train()\n",
    "\n",
    "            batch_loss = 0\n",
    "\n",
    "            for idx, batch in enumerate(train_dataloder):\n",
    "\n",
    "                batch_inputs = [data[0] for data in batch]\n",
    "                batch_sqls = [data[1] for data in batch]\n",
    "\n",
    "                if DEBUG_FLAG:\n",
    "                    if epoch == 0 and idx == 0:\n",
    "                        print(f\"batch_inputs - {type(batch_inputs)} {len(batch_inputs)}\")\n",
    "                        print(f\"batch_sqls - {type(batch_sqls)} {len(batch_sqls)}\")\n",
    "\n",
    "#                 for temp_i, temp in enumerate(batch_inputs):\n",
    "#                     print(f\"batch_inputs - {batch_inputs[temp_i]}\")\n",
    "#                     print(f\"batch_sqls - {batch_sqls[temp_i]}\")\n",
    "\n",
    "                tokenized_inputs = bert_tokenizer(batch_inputs,\n",
    "                                                  add_special_tokens=True,\n",
    "                                                  padding=\"max_length\", #True,\n",
    "                                                  max_length=max_input_length,\n",
    "                                                  #pad_to_max_length=True,\n",
    "                                                  return_tensors='pt',\n",
    "                                                  truncation=True)\n",
    "\n",
    "                encoder_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
    "                encoder_input_attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "#                 print(f\"encoder_input_ids - {encoder_input_ids}\")\n",
    "                tokenized_outputs = t5_tokenizer(batch_sqls,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 padding=\"max_length\", #True,\n",
    "                                                 max_length=max_output_length,\n",
    "                                                 #pad_to_max_length=True,\n",
    "                                                 return_tensors='pt',\n",
    "                                                 truncation=True)\n",
    "\n",
    "\n",
    "                decoder_input_ids = tokenized_outputs[\"input_ids\"].to(device)\n",
    "                # replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "                decoder_input_ids[decoder_input_ids == t5_tokenizer.pad_token_id] = -100\n",
    "                decoder_attention_mask = tokenized_outputs[\"attention_mask\"].to(device)\n",
    "#                 labels = None #tokenized_outputs[\"attention_mask\"].to(device)\n",
    "\n",
    "#                 print(f\"decoder_input_ids - {decoder_input_ids}\")\n",
    "\n",
    "                if DEBUG_FLAG and epoch == 0 and idx == 0:\n",
    "                    print(f\"encoder_input_ids - {encoder_input_ids.size()}\")\n",
    "                    print(f\"encoder_input_attention_mask - {encoder_input_attention_mask.size()}\")\n",
    "                    print(f\"decoder_input_ids - {decoder_input_ids.size()}\")\n",
    "                    print(f\"decoder_attention_mask - {decoder_attention_mask.size()}\")\n",
    "\n",
    "                # Clear gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # # graph\n",
    "                # g  = create_graph(encoder_input_ids)\n",
    "\n",
    "                model_output = model(encoder_input_ids,\n",
    "                               encoder_input_attention_mask,\n",
    "                               decoder_input_ids,\n",
    "                               decoder_attention_mask)\n",
    "#                                labels=labels)\n",
    "\n",
    "                output = model_output[\"logits\"]\n",
    "#                 print(f\"output - {output.size()}\")\n",
    "#                 print(f\"decoder_input_ids - {decoder_input_ids.size()}\")\n",
    "\n",
    "                output_resize = output.view(output.shape[0]*output.shape[1], output.shape[2])\n",
    "                decoder_input_ids_resize = decoder_input_ids.view(decoder_input_ids.shape[0]*decoder_input_ids.shape[1])\n",
    "\n",
    "#                 print(f\"output_resize - {output_resize.size()}\")\n",
    "#                 print(f\"decoder_input_ids_resize - {decoder_input_ids_resize.size()}\")\n",
    "\n",
    "#                 loss = criterion(output_resize, decoder_input_ids_resize)\n",
    "#                 batch_loss += loss.item()\n",
    "\n",
    "#                 print(f\"output - {model_output}\")\n",
    "                loss = model_output[\"loss\"]\n",
    "                batch_loss += loss\n",
    "\n",
    "#                 predicted_classes = torch.argmax(output_resize, dim=-1)\n",
    "\n",
    "#                 print(f\"output_resize - {predicted_classes.size} - {predicted_classes}\")\n",
    "#                 print(f\"decoder_input_ids_resize - {decoder_input_ids_resize}\")\n",
    "\n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                break\n",
    "\n",
    "            batch_loss /= len(train_dataloder)\n",
    "            losses['train'][epoch] = f\"{batch_loss:.3f}\"\n",
    "            #progress bar\n",
    "            tr.set_postfix({\"epoch_num\":epoch,\n",
    "                            \"loss\":f\"{batch_loss:.10f}\"})\n",
    "\n",
    "            with open(os.path.join(models_directory, \"loss.json\"), 'w') as f:\n",
    "                json.dump(losses, f)\n",
    "\n",
    "            writer.add_scalar('Training loss', batch_loss, global_step=epoch+1)\n",
    "            # save models\n",
    "            # if (epoch > 3 and epoch % 5 == 0):\n",
    "            torch.save(model, os.path.join(models_directory, f\"model_{epoch}\"))\n",
    "    torch.save(model, os.path.join(models_directory, f\"model_last_{epoch}\"))\n",
    "    print(f\"saved: {models_directory} model_last_{epoch}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T04:28:51.432368100Z",
     "start_time": "2023-05-12T04:28:51.407754700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches - 3152\n",
      "Tokenizers loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Otimizer - Adam\n",
      "CrossEntropyLoss initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_inputs - <class 'list'> 2\n",
      "batch_sqls - <class 'list'> 2\n",
      "encoder_input_ids - torch.Size([2, 43])\n",
      "encoder_input_attention_mask - torch.Size([2, 43])\n",
      "decoder_input_ids - torch.Size([2, 127])\n",
      "decoder_attention_mask - torch.Size([2, 127])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_output - torch.Size([2, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Define hyperparameters\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_filepath\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../data/resdsql_pre/preprocessed_dataset_train.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m      \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m#32\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m      \u001B[49m\u001B[43mbert_hidden_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m768\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m      \u001B[49m\u001B[43mt5_hidden_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m      \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m#300\u001B[39;49;00m\n\u001B[0;32m      9\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmax_input_length\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m43\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmax_output_length\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m127\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m      \u001B[49m\u001B[43mbert_model\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbert-base-uncased\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m      \u001B[49m\u001B[43mt5_model\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mt5-small\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(train_filepath, batch_size, bert_hidden_size, t5_hidden_size, lr, num_epochs, max_input_length, max_output_length, bert_model, t5_model)\u001B[0m\n\u001B[0;32m    107\u001B[0m                 optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m    109\u001B[0m                 \u001B[38;5;66;03m# # graph\u001B[39;00m\n\u001B[0;32m    110\u001B[0m                 \u001B[38;5;66;03m# g  = create_graph(encoder_input_ids)\u001B[39;00m\n\u001B[1;32m--> 112\u001B[0m                 model_output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mencoder_input_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m                               \u001B[49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;66;03m#                                labels=labels)\u001B[39;00m\n\u001B[0;32m    118\u001B[0m                 output \u001B[38;5;241m=\u001B[39m model_output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mEncoderDecoder.forward\u001B[1;34m(self, input_ids, input_mask, decoder_input_ids, decoder_attention_mask)\u001B[0m\n\u001B[0;32m     42\u001B[0m         \u001B[38;5;66;03m# Transform BERT output to T5 input shape\u001B[39;00m\n\u001B[0;32m     43\u001B[0m         t5_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(bert_output)\n\u001B[1;32m---> 44\u001B[0m         graph_g \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_graph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt5_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m DEBUG_FLAG: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert_output linear - \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt5_input\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m#         t5_input = t5_input.unsqueeze(1).repeat(1, self.max_output_length, 1)\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m#         if DEBUG_FLAG: print(f\"bert_output linear unsqueeze - {t5_input.size()}\")\u001B[39;00m\n",
      "File \u001B[1;32mE:\\GitFolder\\cs685\\TextToSQL_AdvNLP\\notebooks\\graph.py:24\u001B[0m, in \u001B[0;36mcreate_graph\u001B[1;34m(embeddings)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \n\u001B[0;32m     14\u001B[0m \u001B[38;5;124;03m:param embeddings: Normal input embeddings\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m:return: graph which can be given as an input for GNN\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# # Tokenize NL statements and add special tokens\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# input_ids = tokenizer.batch_encode_plus(nl_statements, add_special_tokens=True, return_tensors='pt')['input_ids']\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# # Generate T5 embeddings for NL statements\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m#     embeddings = t5_model(input_ids)[0].squeeze(1)\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Compute similarity matrix based on cosine similarity\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m similarity_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Threshold similarity matrix to create edges\u001B[39;00m\n\u001B[0;32m     26\u001B[0m threshold \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(similarity_matrix) \u001B[38;5;241m+\u001B[39m np\u001B[38;5;241m.\u001B[39mstd(similarity_matrix)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:970\u001B[0m, in \u001B[0;36mTensor.__array__\u001B[1;34m(self, dtype)\u001B[0m\n\u001B[0;32m    968\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(Tensor\u001B[38;5;241m.\u001B[39m__array__, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m    969\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 970\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    971\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    972\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mastype(dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "train(train_filepath = \"../data/resdsql_pre/preprocessed_dataset_train.json\",\n",
    "      batch_size = 2, #32\n",
    "      bert_hidden_size = 768,\n",
    "      t5_hidden_size = 512,\n",
    "      lr = 1e-4,\n",
    "      num_epochs = 10, #300\n",
    "      max_input_length = 43,\n",
    "      max_output_length = 127,\n",
    "      bert_model = 'bert-base-uncased',\n",
    "      t5_model = 't5-small')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T04:29:00.560253600Z",
     "start_time": "2023-05-12T04:28:53.473525400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model class must be defined somewhere\n",
    "model_path = os.path.join(os.getcwd(), \".\\\\models\\\\BERT_T5_lr0.0001_bs2_bert-base-uncased_t5-small\\\\model_last_29\")\n",
    "\n",
    "model2 = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model2.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "dev_filepath = \"../data/resdsql_pre/preprocessed_dataset_test.json\"\n",
    "batch_size = 1\n",
    "max_input_length = 43\n",
    "bert_model = 'bert-base-uncased'\n",
    "t5_model = 't5-small'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "# initialize tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
    "\n",
    "# if isinstance(tokenizer, T5TokenizerFast):\n",
    "#     tokenizer.add_tokens([AddedToken(\" <=\"), AddedToken(\" <\")])\n",
    "\n",
    "dev_dataset = Text2SQLDataset(\n",
    "            dir_ = dev_filepath,\n",
    "            mode = \"train\")\n",
    "\n",
    "dev_dataloder = DataLoader(\n",
    "        dev_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = False,\n",
    "        collate_fn = lambda x: x,\n",
    "        drop_last = False\n",
    "    )\n",
    "\n",
    "# initialize model\n",
    "\n",
    "model2.eval()\n",
    "predict_sqls = []\n",
    "# for batch in tqdm(dev_dataloder):\n",
    "for idx, batch in enumerate(dev_dataloder):\n",
    "    batch_inputs = [data[0] for data in batch]\n",
    "    batch_db_ids = [data[1] for data in batch]\n",
    "    batch_tc_original = [data[2] for data in batch]\n",
    "\n",
    "    tokenized_inputs = bert_tokenizer(batch_inputs,\n",
    "                                      add_special_tokens=True,\n",
    "                                      padding=\"max_length\", #True,\n",
    "                                      max_length=max_input_length,\n",
    "                                      #pad_to_max_length=True,\n",
    "                                      return_tensors='pt',\n",
    "                                      truncation=True)\n",
    "\n",
    "    encoder_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
    "    encoder_input_attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    print(f\"encoder_input_ids - {encoder_input_ids.size()}\")\n",
    "    print(f\"encoder_input_attention_mask - {encoder_input_attention_mask.size()}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model2.predict(encoder_input_ids, encoder_input_attention_mask,\n",
    "                                       batch_size=1, t5_tokenizer=t5_tokenizer)\n",
    "\n",
    "#         model_outputs = model_outputs.view(len(batch_inputs), opt.num_return_sequences, model_outputs.shape[1])\n",
    "\n",
    "#         predict_sqls += decode_sqls(\n",
    "#                                     opt.db_path,\n",
    "#                                     model_outputs,\n",
    "#                                     batch_db_ids,\n",
    "#                                     batch_inputs,\n",
    "#                                     tokenizer,\n",
    "#                                     batch_tc_original\n",
    "#                                     )\n",
    "    break\n",
    "\n",
    "\n",
    "# new_dir = \"/\".join(opt.output.split(\"/\")[:-1]).strip()\n",
    "# if new_dir != \"\":\n",
    "#     os.makedirs(new_dir, exist_ok = True)\n",
    "\n",
    "# # save results\n",
    "# with open(opt.output, \"w\", encoding = 'utf-8') as f:\n",
    "#     for pred in predict_sqls:\n",
    "#         f.write(pred + \"\\n\")\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(\"Text-to-SQL inference spends {}s.\".format(end_time-start_time))\n",
    "\n",
    "# if opt.mode == \"eval\":\n",
    "#     # initialize evaluator\n",
    "#     evaluator = EvaluateTool()\n",
    "#     evaluator.register_golds(opt.original_dev_filepath, opt.db_path)\n",
    "#     spider_metric_result = evaluator.evaluate(predict_sqls)\n",
    "#     print('exact_match score: {}'.format(spider_metric_result[\"exact_match\"]))\n",
    "#     print('exec score: {}'.format(spider_metric_result[\"exec\"]))\n",
    "\n",
    "#     return spider_metric_result[\"exact_match\"], spider_metric_result[\"exec\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
