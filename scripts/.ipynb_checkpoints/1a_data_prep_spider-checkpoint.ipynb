{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a64a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# infiles_data_final = {'train': '/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_combined_train.json',   \n",
    "#                 'dev':'/Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider_data/dev.json',\n",
    "#                 'schema': '/Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider_data/tables.json',\n",
    "#                 'test': '/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_combined_test.json'\n",
    "# }\n",
    "\n",
    "\n",
    "infiles_data_final = {'train': '/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_train.json',   \n",
    "                'dev':'/Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider_data/dev.json',\n",
    "                'schema': '/Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider_data/tables.json',\n",
    "                'test': '/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_test.json'\n",
    "}\n",
    "\n",
    "prefix_data_final = '/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/baseline'\n",
    "infiles = {}\n",
    "infiles[\"data_final\"] = [infiles_data_final, prefix_data_final]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5120118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_NUM_SYMBOL = \"{value}\"\n",
    "\n",
    "def strip_table(table):\n",
    "    column_types = table['column_types']\n",
    "    table_names_original = [cn.lower() for cn in table['table_names_original']]\n",
    "    table_names = [cn.lower() for cn in table['table_names']]\n",
    "    column_names = [cn.lower() for i, cn in table['column_names']]\n",
    "    column_names_original = [cn.lower() for i, cn in table['column_names_original']]\n",
    "    return [table_names_original, table_names, column_names_original, column_names, column_types]\n",
    "\n",
    "def strip_nl(nl):\n",
    "    '''\n",
    "    return keywords of nl query\n",
    "    '''\n",
    "    nl_keywords = []\n",
    "    nl = nl.strip()\n",
    "    nl = nl.replace(\";\",\" ; \").replace(\",\", \" , \").replace(\"?\", \" ? \").replace(\"\\t\",\" \")\n",
    "    nl = nl.replace(\"(\", \" ( \").replace(\")\", \" ) \")\n",
    "    \n",
    "    str_1 = re.findall(\"\\\"[^\\\"]*\\\"\", nl)\n",
    "    str_2 = re.findall(\"\\'[^\\']*\\'\", nl)\n",
    "    float_nums = re.findall(\"[-+]?\\d*\\.\\d+\", nl)\n",
    "    \n",
    "#     values = str_1 + str_2 + float_nums\n",
    "#     for val in values:\n",
    "#         nl = nl.replace(val.strip(), VALUE_NUM_SYMBOL)\n",
    "    \n",
    "    \n",
    "    raw_keywords = nl.strip().split()\n",
    "    for tok in raw_keywords:\n",
    "        if \".\" in tok:\n",
    "            to = tok.replace(\".\", \" . \").split()\n",
    "            to = [t.lower() for t in to if len(t)>0]\n",
    "            nl_keywords.extend(to)\n",
    "        elif \"'\" in tok and tok[0]!=\"'\" and tok[-1]!=\"'\":\n",
    "            to = word_tokenize(tok)\n",
    "            to = [t.lower() for t in to if len(t)>0]\n",
    "            nl_keywords.extend(to)      \n",
    "        elif len(tok) > 0:\n",
    "            nl_keywords.append(tok.lower())\n",
    "    return nl_keywords\n",
    "\n",
    "\n",
    "def strip_query(query):\n",
    "    '''\n",
    "    return keywords of sql query\n",
    "    '''\n",
    "    query_keywords = []\n",
    "    query = query.strip().replace(\";\",\"\").replace(\"\\t\",\"\")\n",
    "    query = query.replace(\"(\", \" ( \").replace(\")\", \" ) \")\n",
    "    query = query.replace(\">=\", \" >= \").replace(\"<=\", \" <= \").replace(\"!=\", \" != \").replace(\"=\", \" = \")\n",
    "\n",
    "    \n",
    "    # then replace all stuff enclosed by \"\" with a numerical value to get it marked as {VALUE}\n",
    "    str_1 = re.findall(\"\\\"[^\\\"]*\\\"\", query)\n",
    "    str_2 = re.findall(\"\\'[^\\']*\\'\", query)\n",
    "    \n",
    "#     values = str_1 + str_2\n",
    "#     for val in values:\n",
    "#         query = query.replace(val.strip(), VALUE_NUM_SYMBOL)\n",
    "\n",
    "    query_tokenized = query.split()\n",
    "    float_nums = re.findall(\"[-+]?\\d*\\.\\d+\", query)\n",
    "#     query_tokenized = [VALUE_NUM_SYMBOL if qt in float_nums else qt for qt in query_tokenized]\n",
    "    query = \" \".join(query_tokenized)\n",
    "    int_nums = [i.strip() for i in re.findall(\"[^tT]\\d+\", query)]\n",
    "\n",
    "    \n",
    "#     query_tokenized = [VALUE_NUM_SYMBOL if qt in int_nums else qt for qt in query_tokenized]\n",
    "    # print int_nums, query, query_tokenized\n",
    "    \n",
    "    for tok in query_tokenized:\n",
    "        if \".\" in tok:\n",
    "            table = re.findall(\"[Tt]\\d+\\.\", tok)\n",
    "            if len(table)>0:\n",
    "                to = tok.replace(\".\", \" . \").split()\n",
    "                to = [t.lower() for t in to if len(t)>0]\n",
    "                query_keywords.extend(to)\n",
    "            else:\n",
    "                query_keywords.append(tok.lower())\n",
    "\n",
    "        elif len(tok) > 0:\n",
    "            query_keywords.append(tok.lower())\n",
    "    query_keywords = [w for w in query_keywords if len(w)>0]\n",
    "    query_sentence = \" \".join(query_keywords)\n",
    "    query_sentence = query_sentence.replace(\"> =\", \">=\").replace(\"! =\", \"!=\").replace(\"< =\", \"<=\")\n",
    "#     if '>' in query_sentence or '=' in query_sentence:\n",
    "#        print query_sentence\n",
    "    return query_sentence.split()\n",
    "\n",
    "def count_databases(infile_group):\n",
    "    content = set()\n",
    "    with open(infile_group['dev']) as f:\n",
    "        ex_list = json.load(f)\n",
    "        for table_dict in ex_list:\n",
    "            content.add(table_dict[\"db_id\"])\n",
    "    dev_count = len(content)\n",
    "    print(\"the number of dev tables are\", dev_count)\n",
    "    \n",
    "    with open(infile_group['train']) as f:\n",
    "        ex_list = json.load(f)\n",
    "        for table_dict in ex_list:\n",
    "            content.add(table_dict[\"db_id\"])\n",
    "    train_count = len(content) - dev_count\n",
    "    print(\"the number of train tables are\",train_count)\n",
    "    \n",
    "    with open(infile_group['test']) as f: \n",
    "        ex_list = json.load(f)\n",
    "        for table_dict in ex_list:\n",
    "            db_id = table_dict[\"db_id\"]\n",
    "            if db_id not in table_dict:\n",
    "                content.add(db_id)\n",
    "    print(\"the number of total tables are\", len(content))\n",
    "    return content\n",
    "\n",
    "def output_vocab_to_txt(outfile, cnt, min_frequency=0, max_vocab_size=None):\n",
    "    file_obj = open(outfile, 'w')\n",
    "    # output vocab with min_frequency, but the same weight\n",
    "    logging.info(\"Found %d unique tokens in the vocabulary.\", len(cnt))\n",
    "    \n",
    "    # Filter tokens below the frequency threshold\n",
    "    fout_decode_file = open(outfile, 'w')\n",
    "    if min_frequency > 0:\n",
    "        filtered_tokens = [(w, c) for w, c in cnt.most_common()\n",
    "                            if c > min_frequency]\n",
    "        cnt = collections.Counter(dict(filtered_tokens))\n",
    "\n",
    "        logging.info(\"Found %d unique tokens with frequency > %d.\",\n",
    "                     len(cnt), min_frequency)\n",
    "\n",
    "    # Sort tokens by 1. frequency 2. lexically to break ties\n",
    "    word_with_counts = cnt.most_common()\n",
    "    word_with_counts = sorted(\n",
    "        word_with_counts, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    # Take only max-vocab\n",
    "    if max_vocab_size is not None:\n",
    "        word_with_counts = word_with_counts[:max_vocab_size]\n",
    "\n",
    "    all_words = {}\n",
    "\n",
    "    for word, count in word_with_counts:\n",
    "        try:\n",
    "            word = str(word)\n",
    "            if word.strip() in all_words:\n",
    "                pass\n",
    "            else:\n",
    "                all_words[word] = 1\n",
    "                file_obj.write(\"{}\\t{}\\n\".format(word, 1))                \n",
    "        except:\n",
    "                file_obj.write(\"{}\\t{}\\n\".format(word.encode('utf-8'), 1))              \n",
    "    file_obj.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d821aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "# from utils import output_vocab_to_txt, infiles, count_databases, strip_query, strip_nl\n",
    "\n",
    "# parser = argparse.ArgumentParser(\n",
    "#     description=\"Generate vocabulary for a tokenized text file.\")\n",
    "# parser.add_argument(\n",
    "#     \"--min_frequency\",\n",
    "#     dest=\"min_frequency\",\n",
    "#     type=int,\n",
    "#     default=0,\n",
    "#     help=\"Minimum frequency of a word to be included in the vocabulary.\")\n",
    "# parser.add_argument(\n",
    "#     \"--max_vocab_size\",\n",
    "#     dest=\"max_vocab_size\",\n",
    "#     type=int,\n",
    "#     help=\"Maximum number of tokens in the vocabulary\")\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7442050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode_Query(infile_group, outfile, infile, output=False):\n",
    "    max_nl = 0\n",
    "    cnt = collections.Counter()\n",
    "    infile = infile_group[infile]\n",
    "    if output:\n",
    "        outfile = open(outfile, 'w')\n",
    "    \n",
    "    cnt = collections.Counter()    \n",
    "    with open(infile) as f:\n",
    "        ex_list = json.load(f)\n",
    "        for nl_sql_dict in ex_list:\n",
    "            tokens = strip_nl(nl_sql_dict[\"question\"])                    \n",
    "            cnt.update(tokens)\n",
    "            max_nl = max(max_nl, len(tokens))\n",
    "            token_sentence = \" \".join(tokens)\n",
    "            if output:\n",
    "                try:\n",
    "                    outfile.write(\"{}\\n\".format(token_sentence))\n",
    "                except:\n",
    "                    outfile.write(\"{}\\n\".format(token_sentence.encode('utf-8')))\n",
    "                \n",
    "        if output:\n",
    "            outfile.close()\n",
    "    print(\"max length nl of\", infile, \"is\", max_nl)\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def get_mask(infile_group, outfile, infile, vocabfile, output=True):\n",
    "    \n",
    "    \n",
    "    \n",
    "    _, used_databases, db_dict_rev = get_schema_vocab(infile_group, \"schema\")\n",
    "    key_words = sql_key_words()\n",
    "    infile_name = infile_group[infile]\n",
    "    if output:\n",
    "        outfile = open(outfile, 'w')\n",
    "    vocab_ls = []\n",
    "    with open(vocabfile, \"r\") as vocab:\n",
    "        for line in vocab:\n",
    "            items = line.split()\n",
    "            vocab_ls.append(items[0])\n",
    "    print(f\"decode vocab length is {vocabfile}\", len(vocab_ls))\n",
    "    with open(infile_name) as f:\n",
    "        ex_list = json.load(f)\n",
    "        for nl_sql_dict in ex_list:\n",
    "            if infile != 'train':\n",
    "                db_id = nl_sql_dict[\"db_id\"]\n",
    "\n",
    "                binary = []\n",
    "                for item in vocab_ls:\n",
    "                    # print db_id\n",
    "                    if item in key_words:\n",
    "                        binary.append(\"1\")\n",
    "                    elif (item in db_dict_rev ) and (db_id in db_dict_rev[item]):\n",
    "                        binary.append(\"1\")\n",
    "                    else:\n",
    "                        binary.append(\"0\")\n",
    "                binary.extend([\"1\"] * 5)\n",
    "                if output:\n",
    "                    outfile.write(\"{}\\n\".format(\" \".join(binary)))\n",
    "            elif output:\n",
    "                outfile.write(\"{}\\n\".format(\"1\"))\n",
    "                \n",
    "\n",
    "        if output:\n",
    "            outfile.close()\n",
    "        \n",
    "    \n",
    "        \n",
    "def get_decode_SQL(infile_group, outfile, infile, output=False, outputdb=False):\n",
    "    max_sql = 0\n",
    "    \n",
    "    cnt = collections.Counter()\n",
    "    infile = infile_group[infile]\n",
    "    if output:\n",
    "        outfile = open(outfile, 'w')\n",
    "    \n",
    "    with open(infile) as f:\n",
    "        ex_list = json.load(f)\n",
    "        for nl_sql_dict in ex_list:\n",
    "            tokens = strip_query(nl_sql_dict[\"query\"])       \n",
    "            max_sql = max(max_sql, len(tokens))\n",
    "            cnt.update(tokens)\n",
    "            token_sentence = \" \".join(tokens)\n",
    "            if output and not outputdb:\n",
    "                try:\n",
    "                    outfile.write(\"{}\\n\".format(token_sentence))\n",
    "                except:\n",
    "                    outfile.write(\"{}\\n\".format(token_sentence.encode('utf-8')))\n",
    "            elif output and outputdb:\n",
    "                try:\n",
    "                    outfile.write(\"{}\\t{}\\n\".format(nl_sql_dict[\"query\"].lower().replace(\"\\t\", \" \"), nl_sql_dict[\"db_id\"]))\n",
    "                except:\n",
    "                    outfile.write(\"{}\\t{}\\n\".format(nl_sql_dict[\"query\"].encode('utf-8').lower().replace(\"\\t\", \" \"), nl_sql_dict[\"db_id\"]))\n",
    "        if output:\n",
    "            outfile.close()\n",
    "    print(\"max sql length of\", infile, \"is\", max_sql)\n",
    "    return cnt\n",
    "\n",
    "def get_schema_vocab(infile_group, infile):\n",
    "    used_databases = set()\n",
    "    cnt = collections.Counter()\n",
    "    db_dict_rev = {}\n",
    "    with open(infile_group[infile]) as f:\n",
    "        ex_list = json.load(f)\n",
    "        for table_dict in ex_list:\n",
    "            db_id = table_dict[\"db_id\"]\n",
    "            if db_id not in used_databases:\n",
    "                used_databases.add(db_id)\n",
    "            new_tokens = []\n",
    "            column_names = table_dict[\"column_names_original\"]\n",
    "            table_names = table_dict[\"table_names_original\"]\n",
    "            for item in column_names:\n",
    "                new_tokens.append(item[1].lower())\n",
    "            for table_name in table_names:\n",
    "                new_tokens.append(table_name.lower())\n",
    "            cnt.update(new_tokens)   \n",
    "            \n",
    "            # build look up\n",
    "            for tok in new_tokens:\n",
    "                if tok not in db_dict_rev:\n",
    "                    db_dict_rev[tok] = [db_id]\n",
    "                else:\n",
    "                    if db_id not in db_dict_rev[tok]:\n",
    "                        db_dict_rev[tok].append(db_id)\n",
    "                    \n",
    "                \n",
    "    return cnt, used_databases, db_dict_rev\n",
    "\n",
    "\n",
    "\n",
    "def sql_key_words():\n",
    "    cnt = collections.Counter()\n",
    "    cnt.update([\"t\"+str(i+1) for i in range(10)])\n",
    "    cnt.update([\".\", \",\", \"(\", \")\", \"in\", \"not\", \"and\", \"between\", \"or\", \"where\",\n",
    "                \"except\", \"union\", \"intersect\",\n",
    "                \"group\", \"by\", \"order\", \"limit\", \"having\",\"asc\", \"desc\",\n",
    "                \"count\", \"sum\", \"avg\", \"max\", \"min\",\n",
    "                \"{value}\",\n",
    "               \"<\", \">\", \"=\", \"!=\", \">=\", \"<=\",\n",
    "                \"like\",\n",
    "                \"distinct\",\"*\",\n",
    "                \"join\", \"on\", \"as\", \"select\", \"from\"\n",
    "               ])\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def get_decode_vocab_no_weight(infile_group, outfile):\n",
    "\n",
    "    cnt, _, db_dict_rev = get_schema_vocab(infile_group, \"schema\")\n",
    "    cnt.update(sql_key_words())\n",
    "    output_vocab_to_txt(outfile, cnt)\n",
    "\n",
    "\n",
    "\n",
    "def get_encode_vocab_no_weight(infile_group, outfile):\n",
    "    cnt = collections.Counter()\n",
    "    cnt.update(get_encode_Query(infile_group, None, 'train', False))\n",
    "    cnt.update(get_encode_Query(infile_group, None, 'dev', False))   \n",
    "    cnt.update(get_schema_vocab(infile_group, \"schema\")[0])\n",
    "    output_vocab_to_txt(outfile, cnt)\n",
    "\n",
    "    \n",
    "def decode_encode_copy(infile_group, outfile):\n",
    "    cnt = collections.Counter()\n",
    "\n",
    "    cnt.update(get_encode_Query(infile_group, None, 'train', False))\n",
    "    cnt.update(get_encode_Query(infile_group, None, 'dev', False))   \n",
    "    cnt.update(get_schema_vocab(infile_group, \"schema\")[0])\n",
    "    cnt.update(sql_key_words())\n",
    "\n",
    "    output_vocab_to_txt(outfile, cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2714307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of dev tables are 20\n",
      "the number of train tables are 129\n",
      "the number of total tables are 160\n",
      "max sql length of /Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider_data/dev.json is 88\n",
      "max sql length of /Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_test.json is 81\n",
      "max sql length of /Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_train.json is 127\n",
      "max sql length of /Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider_data/dev.json is 88\n",
      "max sql length of /Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_test.json is 81\n",
      "max sql length of /Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_train.json is 127\n",
      "max length nl of /Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_test.json is 39\n",
      "max length nl of /Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider_data/dev.json is 33\n",
      "max length nl of /Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_train.json is 43\n",
      "max length nl of /Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_train.json is 43\n",
      "max length nl of /Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider_data/dev.json is 33\n",
      "decode vocab length is /Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/baseline/decode_vocab.txt 2699\n",
      "decode vocab length is /Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/baseline/decode_vocab.txt 2699\n",
      "decode vocab length is /Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/baseline/decode_vocab.txt 2699\n"
     ]
    }
   ],
   "source": [
    "for key in infiles.keys():\n",
    "    infile_group, prefix = infiles[key]\n",
    "    count_databases(infile_group)\n",
    "    if not os.path.exists(os.path.join(prefix,'dev')):\n",
    "        os.makedirs(os.path.join(prefix,'dev'))\n",
    "    if not os.path.exists(os.path.join(prefix,'train')):\n",
    "        os.makedirs(os.path.join(prefix,'train'))\n",
    "    if not os.path.exists(os.path.join(prefix,'test')):\n",
    "        os.makedirs(os.path.join(prefix,'test'))\n",
    "\n",
    "    get_decode_SQL(infile_group, os.path.join(prefix,'dev', \"dev_decode.txt\"), \"dev\", True)        \n",
    "    get_decode_SQL(infile_group, os.path.join(prefix,'test', \"test_decode.txt\"), \"test\", True)\n",
    "    get_decode_SQL(infile_group, os.path.join(prefix,'train', \"train_decode.txt\"), \"train\", True)\n",
    "\n",
    "    get_decode_SQL(infile_group, os.path.join(prefix,'dev', \"dev_decode_db.txt\"), \"dev\", True, True)       \n",
    "    get_decode_SQL(infile_group, os.path.join(prefix,'test', \"test_decode_db.txt\"), \"test\", True, True)\n",
    "    get_decode_SQL(infile_group, os.path.join(prefix,'train', \"train_decode_db.txt\"), \"train\", True, True)\n",
    "\n",
    "    get_encode_Query(infile_group, os.path.join(prefix,'test', \"test_encode.txt\"), \"test\", True)\n",
    "    get_encode_Query(infile_group, os.path.join(prefix,'dev', \"dev_encode.txt\"), \"dev\", True)\n",
    "    get_encode_Query(infile_group, os.path.join(prefix,'train', \"train_encode.txt\"), \"train\", True)\n",
    "\n",
    "    get_encode_vocab_no_weight(infile_group, os.path.join(prefix, \"encode_vocab.txt\"))\n",
    "\n",
    "    get_decode_vocab_no_weight(infile_group, os.path.join(prefix, \"decode_vocab.txt\"))\n",
    "    get_decode_vocab_no_weight(infile_group, os.path.join(prefix, \"decode_copy_encode_vocab.txt\"))\n",
    "\n",
    "\n",
    "    get_mask(infile_group, os.path.join(prefix, \"test\", \"test_decoder_mask.txt\"), \"test\",os.path.join(prefix, \"decode_vocab.txt\"), True)\n",
    "    get_mask(infile_group, os.path.join(prefix, \"dev\", \"dev_decoder_mask.txt\"), \"dev\",os.path.join(prefix, \"decode_vocab.txt\"), True)\n",
    "    get_mask(infile_group, os.path.join(prefix, \"train\", \"train_decoder_mask.txt\"), \"train\", os.path.join(prefix, \"decode_vocab.txt\"),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a920c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
