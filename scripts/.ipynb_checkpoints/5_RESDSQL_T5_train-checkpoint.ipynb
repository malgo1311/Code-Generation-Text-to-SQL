{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929904b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOUNTING GOOGLE DRIVE\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "wd = '/content/drive/MyDrive/CS 685/cs685_project/notebooks'\n",
    "print(os.listdir(wd))\n",
    "os.chdir(wd)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tokenizers\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd950b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b39341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import torch.optim as optim\n",
    "\n",
    "import transformers\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers.optimization import Adafactor\n",
    "from transformers.trainer_utils import set_seed\n",
    "# from utils.spider_metric.evaluator import EvaluateTool\n",
    "# from utils.load_dataset import Text2SQLDataset\n",
    "from load_dataset import Text2SQLDataset\n",
    "# from utils.text2sql_decoding_utils import decode_sqls, decode_natsqls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6862c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #'file path of test2sql training set.')\n",
    "# train_filepath = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider/baselines/seq2seq_attention_copy/data/datasets/data_final/spider_combined_train.json\"\n",
    "# train_filepath = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/resdsql_pre/preprocessed_dataset.json\" \n",
    "train_filepath = \"../data/resdsql_pre/preprocessed_dataset_train.json\"\n",
    "batch_size = 2 #'input batch size.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56791bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Text2SQLDataset(\n",
    "        dir_ = train_filepath,\n",
    "        mode = \"train\")\n",
    "\n",
    "train_dataloder = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = True,\n",
    "        collate_fn = lambda x: x,\n",
    "        drop_last = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd62ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloder:\n",
    "    batch_inputs = [data[0] for data in batch]\n",
    "    batch_sqls = [data[1] for data in batch]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "050db96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"how many participants belong to the type 'Organizer' ?\",\n",
       "  'compute the number of products with a price larger than or equal to $180 . '],\n",
       " [\"select count ( * ) from participants where participant_type_code = 'Organizer'\",\n",
       "  'select count ( * ) from products where price >= 180'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs, batch_sqls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2849a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_len = 43\n",
    "max_decoder_len = 127\n",
    "\n",
    "# max_encoder_len += 2\n",
    "# max_decoder_len += 2\n",
    "\n",
    "gradient_descent_step = 4 #'perform gradient descent per \"gradient_descent_step\" steps.')\n",
    "# device = \"2\" #'the id of used GPU device.')\n",
    "learning_rate = 3e-5 #'learning rate.')\n",
    "epochs = 1 #'training epochs.')\n",
    "seed = 42 #'random seed.')\n",
    "save_path = \"models/text2sql\" #'save path of best fine-tuned text2sql model.')\n",
    "tensorboard_save_path= \"tb/text2sql\" #'save path of tensorboard log.')\n",
    "'''\n",
    "pre-trained model name. \n",
    "options: \n",
    "    t5-base, https://huggingface.co/t5-base;\n",
    "    t5-large, https://huggingface.co/t5-large;\n",
    "    t5-3b, https://huggingface.co/t5-3b;\n",
    ")'''\n",
    "\n",
    "model_name_or_path = \"t5-small\" #\"t5-3b\",\n",
    "use_adafactor = True #'whether to use adafactor optimizer.')\n",
    "mode = \"train\" #'trian, eval or test.')\n",
    "# dev_filepath = \"data/preprocessed_data/resdsql_dev.json\" #'file path of test2sql dev set.')\n",
    "# original_dev_filepath = \"data/spider/dev.json\" #'file path of the original dev set (for registing evaluator).')\n",
    "db_path = \"database\" #file path of database.')\n",
    "# tables_for_natsql = \"NatSQL/NatSQLv1_6/tables_for_natsql.json\" #'file path of tables_for_natsql.json.')\n",
    "num_beams = 8 #'beam size in model.generate() function.')\n",
    "num_return_sequences = 8 #'the number of returned sequences in model.generate() function (num_return_sequences <= num_beams).')\n",
    "\n",
    "output = \"predicted_sql.txt\" #\"save file of the predicted sqls.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65f66c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "set_seed(seed)\n",
    "writer = SummaryWriter(tensorboard_save_path)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70154395",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2sql_tokenizer = T5TokenizerFast.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    add_prefix_space = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52323c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(text2sql_tokenizer, T5TokenizerFast):\n",
    "    text2sql_tokenizer.add_tokens([AddedToken(\" <=\"), AddedToken(\" <\")])\n",
    "\n",
    "train_dataset = Text2SQLDataset(\n",
    "    dir_ = train_filepath,\n",
    "    mode = \"train\")\n",
    "\n",
    "train_dataloder = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = True,\n",
    "    collate_fn = lambda x: x,\n",
    "    drop_last = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64f52785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing text2sql model.\n",
      "finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"initializing text2sql model.\")\n",
    "# initialize model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "model.resize_token_embeddings(len(text2sql_tokenizer))\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "print(\"finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a0baa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use Adafactor!\n"
     ]
    }
   ],
   "source": [
    "# warm up steps (10% training step)\n",
    "num_warmup_steps = int(0.1*epochs*len(train_dataset)/batch_size)\n",
    "# total training steps\n",
    "num_training_steps = int(epochs*len(train_dataset)/batch_size)\n",
    "# save checkpoint\n",
    "num_checkpoint_steps = 500\n",
    "\n",
    "print(\"Let's use Adafactor!\")\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    scale_parameter=False, \n",
    "    relative_step=False, \n",
    "    clip_threshold = 1.0,\n",
    "    warmup_init=False)\n",
    "\n",
    "#     print(\"Let's use AdamW!\")\n",
    "#     optimizer = optim.AdamW(\n",
    "#         model.parameters(), \n",
    "#         lr = learning_rate)\n",
    "\n",
    "scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = num_warmup_steps,\n",
    "    num_training_steps = num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f60436a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                   | 0/1 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m     decoder_labels \u001b[38;5;241m=\u001b[39m decoder_labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     63\u001b[0m     decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 65\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_input_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m loss \u001b[38;5;241m=\u001b[39m model_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     74\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1718\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1715\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1718\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1733\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1088\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1076\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1077\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     )\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1088\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:723\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:633\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    623\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    631\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    632\u001b[0m ):\n\u001b[0;32m--> 633\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncDecAttention(\n\u001b[1;32m    635\u001b[0m         normed_hidden_states,\n\u001b[1;32m    636\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    644\u001b[0m     )\n\u001b[1;32m    645\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Aishwarya/Learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_step = 0\n",
    "# initialize array of losses \n",
    "losses = {'train': {}, \"val\": {}}\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "with trange(epochs) as tr:\n",
    "    for epoch in tr:\n",
    "#         print(f\"This is epoch {epoch+1}.\")\n",
    "        batch_loss = 0\n",
    "    \n",
    "        for idx, batch in enumerate(train_dataloder):\n",
    "            train_step += 1\n",
    "\n",
    "            batch_inputs = [data[0] for data in batch]\n",
    "            batch_sqls = [data[1] for data in batch]\n",
    "    #             batch_db_ids = [data[2] for data in batch] # unused\n",
    "    #             batch_tc_original = [data[3] for data in batch] # unused\n",
    "\n",
    "    #         if epoch == 0 and idx == 0:\n",
    "    #             for batch_id in range(len(batch_inputs)):\n",
    "    #                 print(f\"batch_inputs - {batch_inputs[batch_id]}\")\n",
    "    #                 print(f\"batch_sqls - {batch_sqls[batch_id]}\")\n",
    "    # #                 print(\"----------------------\")\n",
    "\n",
    "            tokenized_inputs = text2sql_tokenizer(\n",
    "                batch_inputs, \n",
    "                padding = \"max_length\",\n",
    "                return_tensors = \"pt\",\n",
    "                max_length = max_encoder_len, #512,\n",
    "                truncation = True\n",
    "            )\n",
    "\n",
    "            with text2sql_tokenizer.as_target_tokenizer():\n",
    "                tokenized_outputs = text2sql_tokenizer(\n",
    "                    batch_sqls, \n",
    "                    padding = \"max_length\", \n",
    "                    return_tensors = 'pt',\n",
    "                    max_length = max_decoder_len, #256,\n",
    "                    truncation = True\n",
    "                )\n",
    "\n",
    "            encoder_input_ids = tokenized_inputs[\"input_ids\"]\n",
    "            encoder_input_attention_mask = tokenized_inputs[\"attention_mask\"]\n",
    "\n",
    "            decoder_labels = tokenized_outputs[\"input_ids\"]\n",
    "            # replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "            decoder_labels[decoder_labels == text2sql_tokenizer.pad_token_id] = -100\n",
    "            decoder_attention_mask = tokenized_outputs[\"attention_mask\"]\n",
    "\n",
    "    #         if idx == 0:\n",
    "    #             print(f\"tokenized_inputs - {tokenized_inputs}\")\n",
    "    #             print(f\"tokenized_outputs - {tokenized_outputs}\")\n",
    "    # #             print(f\"encoder_input_ids - {encoder_input_ids}\")\n",
    "    #             print(f\"encoder_input_attention_mask - {encoder_input_attention_mask}\")\n",
    "    #             print(f\"decoder_labels - {decoder_labels}\")\n",
    "    #             print(f\"decoder_attention_mask - {decoder_attention_mask}\")\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                encoder_input_ids = encoder_input_ids.cuda()\n",
    "                encoder_input_attention_mask = encoder_input_attention_mask.cuda()\n",
    "                decoder_labels = decoder_labels.cuda()\n",
    "                decoder_attention_mask = decoder_attention_mask.cuda()\n",
    "\n",
    "            model_outputs = model(\n",
    "                input_ids = encoder_input_ids,\n",
    "                attention_mask = encoder_input_attention_mask,\n",
    "                labels = decoder_labels,\n",
    "                decoder_attention_mask = decoder_attention_mask,\n",
    "                return_dict = True\n",
    "            )\n",
    "\n",
    "            loss = model_outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            \n",
    "            batch_loss += loss\n",
    "\n",
    "#             if scheduler is not None:\n",
    "#                 scheduler.step()\n",
    "\n",
    "            if writer is not None:\n",
    "                # record training loss (tensorboard)\n",
    "                writer.add_scalar('train loss', loss.item(), train_step)\n",
    "                # record learning rate (tensorboard)\n",
    "                writer.add_scalar('train lr', optimizer.state_dict()['param_groups'][0]['lr'], train_step)\n",
    "\n",
    "            if train_step % gradient_descent_step == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if train_step % num_checkpoint_steps == 0 and epoch >= 6:\n",
    "                print(f\"At {train_step} training step, save a checkpoint.\")\n",
    "                os.makedirs(save_path, exist_ok = True)\n",
    "                model.save_pretrained(save_directory = save_path + \"/checkpoint-{}\".format(train_step))\n",
    "                text2sql_tokenizer.save_pretrained(save_directory = save_path + \"/checkpoint-{}\".format(train_step))\n",
    "\n",
    "        batch_loss /= len(train_dataloder) \n",
    "        losses['train'][epoch] = f\"{batch_loss:.3f}\"\n",
    "        #progress bar \n",
    "        tr.set_postfix({\"epoch_num\":epoch,\n",
    "                        \"loss\":f\"{batch_loss:.10f}\"})\n",
    "    #         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_id in [42, 363, 1, 0, 58, 1738, 3,  9,  208,  122,   41, 4668,  834, 6254,  3,   61,   45, 6407]:\n",
    "    vocab_word = text2sql_tokenizer.convert_ids_to_tokens(token_id)\n",
    "    print(f\"{token_id} - {vocab_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d4a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test(opt):\n",
    "    set_seed(opt.seed)\n",
    "    print(opt)\n",
    "\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.device\n",
    "\n",
    "    # initialize tokenizer\n",
    "    tokenizer = T5TokenizerFast.from_pretrained(\n",
    "        opt.save_path,\n",
    "        add_prefix_space = True\n",
    "    )\n",
    "    \n",
    "    if isinstance(tokenizer, T5TokenizerFast):\n",
    "        tokenizer.add_tokens([AddedToken(\" <=\"), AddedToken(\" <\")])\n",
    "    \n",
    "    dev_dataset = Text2SQLDataset(\n",
    "        dir_ = opt.dev_filepath,\n",
    "        mode = opt.mode\n",
    "    )\n",
    "\n",
    "    dev_dataloder = DataLoader(\n",
    "        dev_dataset, \n",
    "        batch_size = opt.batch_size, \n",
    "        shuffle = False,\n",
    "        collate_fn = lambda x: x,\n",
    "        drop_last = False\n",
    "    )\n",
    "\n",
    "    # initialize model\n",
    "    model = T5ForConditionalGeneration.from_pretrained(opt.save_path)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    model.eval()\n",
    "    predict_sqls = []\n",
    "    for batch in tqdm(dev_dataloder):\n",
    "        batch_inputs = [data[0] for data in batch]\n",
    "        batch_db_ids = [data[1] for data in batch]\n",
    "        batch_tc_original = [data[2] for data in batch]\n",
    "\n",
    "        tokenized_inputs = tokenizer(\n",
    "            batch_inputs, \n",
    "            return_tensors=\"pt\",\n",
    "            padding = \"max_length\",\n",
    "            max_length = 512,\n",
    "            truncation = True\n",
    "        )\n",
    "        \n",
    "        encoder_input_ids = tokenized_inputs[\"input_ids\"]\n",
    "        encoder_input_attention_mask = tokenized_inputs[\"attention_mask\"]\n",
    "        if torch.cuda.is_available():\n",
    "            encoder_input_ids = encoder_input_ids.cuda()\n",
    "            encoder_input_attention_mask = encoder_input_attention_mask.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_outputs = model.generate(\n",
    "                input_ids = encoder_input_ids,\n",
    "                attention_mask = encoder_input_attention_mask,\n",
    "                max_length = 256,\n",
    "                decoder_start_token_id = model.config.decoder_start_token_id,\n",
    "                num_beams = opt.num_beams,\n",
    "                num_return_sequences = opt.num_return_sequences\n",
    "            )\n",
    "\n",
    "            model_outputs = model_outputs.view(len(batch_inputs), opt.num_return_sequences, model_outputs.shape[1])\n",
    "\n",
    "            predict_sqls += decode_sqls(\n",
    "                opt.db_path, \n",
    "                model_outputs, \n",
    "                batch_db_ids, \n",
    "                batch_inputs, \n",
    "                tokenizer, \n",
    "                batch_tc_original\n",
    "            )\n",
    "\n",
    "    new_dir = \"/\".join(opt.output.split(\"/\")[:-1]).strip()\n",
    "    if new_dir != \"\":\n",
    "        os.makedirs(new_dir, exist_ok = True)\n",
    "    \n",
    "    # save results\n",
    "    with open(opt.output, \"w\", encoding = 'utf-8') as f:\n",
    "        for pred in predict_sqls:\n",
    "            f.write(pred + \"\\n\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Text-to-SQL inference spends {}s.\".format(end_time-start_time))\n",
    "    \n",
    "    if opt.mode == \"eval\":\n",
    "        # initialize evaluator\n",
    "        evaluator = EvaluateTool()\n",
    "        evaluator.register_golds(opt.original_dev_filepath, opt.db_path)\n",
    "        spider_metric_result = evaluator.evaluate(predict_sqls)\n",
    "        print('exact_match score: {}'.format(spider_metric_result[\"exact_match\"]))\n",
    "        print('exec score: {}'.format(spider_metric_result[\"exec\"]))\n",
    "    \n",
    "        return spider_metric_result[\"exact_match\"], spider_metric_result[\"exec\"]\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_option()\n",
    "    if opt.mode in [\"train\"]:\n",
    "        _train(opt)\n",
    "    elif opt.mode in [\"eval\", \"test\"]:\n",
    "        _test(opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
