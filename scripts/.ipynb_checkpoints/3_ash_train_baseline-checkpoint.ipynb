{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "QSbW-vTgDaGN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 17765,
     "status": "ok",
     "timestamp": 1683646908203,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "QSbW-vTgDaGN",
    "outputId": "d1d5c203-8fb2-44d8-f87c-74b84cc27735"
   },
   "outputs": [],
   "source": [
    "# MOUNTING GOOGLE DRIVE\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4NAD9rIfD7XL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1683646941546,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "4NAD9rIfD7XL",
    "outputId": "9a190f82-eab9-40cb-deef-d8e31a7184f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3_ash_train_baseline.ipynb', 'baseline_1.ipynb', 'Untitled0.ipynb', 'check_spider_data.ipynb', 'runs', 'model_0.001_2_0']\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/CS 685/cs685_project/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd = '/content/drive/MyDrive/CS 685/cs685_project/notebooks'\n",
    "print(os.listdir(wd))\n",
    "os.chdir(wd)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3fefe7",
   "metadata": {
    "id": "bf3fefe7"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ce0231",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7151,
     "status": "ok",
     "timestamp": 1683646950573,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "49ce0231",
    "outputId": "e15ce96a-5cfa-43ad-bd7e-07d41aae9813"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, errno\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from utils import save_checkpoint, load_checkpoint\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "# from load_dataset import Text2SQLDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ffed02",
   "metadata": {
    "id": "a9ffed02"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbd13c2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1683647494291,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "bbd13c2d",
    "outputId": "ca812b89-0519-42ee-c0f5-0f612440bea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encoder input - (43, 5)\n",
      "Train decoder input - (129, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 5938,\n",
       " 'max_encoder_len': 43,\n",
       " 'max_decoder_len': 127,\n",
       " 'pad_idx': 1462,\n",
       " 'sos_idx': 1463,\n",
       " 'eos_idx': 1461}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# local\n",
    "# target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/LSTM_encoder_decoder/data/data_final_processed_v2\"\n",
    "# target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/baseline/training_data\"\n",
    "\n",
    "# for colab\n",
    "target_folder = \"../data/baseline/training_data\"\n",
    "\n",
    "## GET DATA\n",
    "#sample data for checking network\n",
    "fol1 = 'train'\n",
    "data_t = 'encode'\n",
    "X_train_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "X_train_np = X_train_np[:5]\n",
    "X_train_np = X_train_np.transpose(1,0)\n",
    "# train_input = np.expand_dims(train_input, axis=-1) \n",
    "print(f'Train encoder input - {X_train_np.shape}')\n",
    "\n",
    "\n",
    "#sample data for checking network\n",
    "fol1 = 'train'\n",
    "data_t = 'decode'\n",
    "Y_train_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "Y_train_np = Y_train_np[:5]\n",
    "Y_train_np = Y_train_np.transpose(1,0)\n",
    "# train_output = np.expand_dims(train_output, axis=-1) \n",
    "print(f'Train decoder input - {Y_train_np.shape}')\n",
    "\n",
    "with open(os.path.join(target_folder, 'data_info.json'), 'r') as fp:\n",
    "    data_info = json.load(fp)\n",
    "    \n",
    "pad_idx = data_info['pad_idx']\n",
    "sos_idx = data_info['sos_idx']\n",
    "vocab_size = data_info['vocab_size']\n",
    "    \n",
    "data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15d0dd19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683647495355,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "15d0dd19",
    "outputId": "371d5311-845f-4905-ad13-d72e7343904d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([43, 5]), torch.Size([129, 5]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert numpy array to tensors\n",
    "X_train = torch.from_numpy(X_train_np).type(torch.int64) #torch.int64, torch.Tensor\n",
    "Y_train = torch.from_numpy(Y_train_np).type(torch.int64)\n",
    "\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ab015",
   "metadata": {
    "id": "1a6ab015"
   },
   "source": [
    "## Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c30349",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683646952923,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "24c30349"
   },
   "outputs": [],
   "source": [
    "# FOR PRINTING INTERMEDIATE TORCH SIZES\n",
    "DEBUG_FLAG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205cf51e",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683646952923,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "205cf51e"
   },
   "outputs": [],
   "source": [
    "class bilstm_encoder(nn.Module):\n",
    "    ''' Encodes time-series sequence '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, emb_size, num_layers = 1, dropout = 0):\n",
    "        \n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X, eg: word embeddings\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(bilstm_encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder: input_size {input_size} - hidden_size {hidden_size} - emb_size {emb_size}\")\n",
    "\n",
    "        # define embeddings\n",
    "        self.embeddings = nn.Embedding(input_size, emb_size)\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = emb_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            bidirectional = True,\n",
    "                            dropout = dropout)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        \n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch) #, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence;\n",
    "        :                              hidden gives the hidden state and cell state for the last\n",
    "        :                              element in the sequence \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        embedded = self.embeddings(x_input)\n",
    "        # embedded size: (seq_len, batch_size, embedding_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder embedded size - {type(embedded)} - {embedded.shape}\")\n",
    "#         embedded = embedded.view(1, 1, -1)\n",
    "#         print(f\"Encoder embedded size - {type(embedded)} - {embedded.shape}\")\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(embedded)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder hidden_state size - {type(self.hidden)} - {self.hidden[0].shape}\")\n",
    "        # lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n",
    "        \n",
    "        return lstm_out, self.hidden     \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        '''\n",
    "        initialize hidden state\n",
    "        : param batch_size:    x_input.shape[1]\n",
    "        : return:              zeroed hidden state and cell state \n",
    "        '''\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1333309f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683646952923,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "1333309f",
    "outputId": "1c50fc4c-9f09-4273-8c90-3feb9dd35e5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([43, 6304, 60]),\n",
       " torch.Size([2, 6304, 30]),\n",
       " torch.Size([2, 6304, 30]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = bilstm_encoder(vocab_size, 30, 20)\n",
    "out, enc_hidden_state = enc.forward(X_train)\n",
    "out.shape, enc_hidden_state[0].shape, enc_hidden_state[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d18bead6",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1683646952923,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "d18bead6"
   },
   "outputs": [],
   "source": [
    "# No bi - (torch.Size([43, 5, 30]), torch.Size([1, 5, 30]), torch.Size([1, 5, 30]))\n",
    "# With bi - (torch.Size([43, 5, 60]), torch.Size([2, 5, 30]), torch.Size([2, 5, 30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d108575",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683646952924,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "4d108575"
   },
   "outputs": [],
   "source": [
    "class bilstm_decoder(nn.Module):\n",
    "    ''' Decodes hidden state output by encoder '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, emb_size, output_size, num_layers = 2, dropout = 0):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(bilstm_decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Encoder: input_size {input_size} - hidden_size {hidden_size} - emb_size {emb_size} - output_size {output_size}\")\n",
    "        \n",
    "        # define embeddings\n",
    "        self.embeddings = nn.Embedding(input_size, emb_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = emb_size,\n",
    "                            hidden_size = hidden_size,\n",
    "                            num_layers = num_layers,\n",
    "                            bidirectional = False,\n",
    "                            dropout = dropout)\n",
    "        # bi is true\n",
    "        # self.linear = nn.Linear(2*hidden_size, output_size)\n",
    "        # num_layers = 1\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        \n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (1, batch_size) #, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence;\n",
    "        :                                   hidden gives the hidden state and cell state for the last\n",
    "        :                                   element in the sequence \n",
    " \n",
    "        '''\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder x_input size - {x_input.shape}\")\n",
    "        x_input = x_input.unsqueeze(0)\n",
    "        # x_input size: (1, batch_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder x_input size - {x_input.shape}\")\n",
    "        \n",
    "        embedded = self.embeddings(x_input)\n",
    "        # embedded size: (1, batch_size, embedding_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder embedded size - {embedded.shape}\")\n",
    "            print(f\"Decoder encoder_hidden_states size - {encoder_hidden_states[0].shape}\")\n",
    "            \n",
    "        lstm_out, self.hidden = self.lstm(embedded, encoder_hidden_states)\n",
    "        # lstm_out size: (1, batch_size, hidden_size)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder lstm_out size - {lstm_out.shape}\")\n",
    "            print(f\"Decoder hidden size - {self.hidden[0].shape}\")\n",
    "        \n",
    "        lstm_out = lstm_out.squeeze(0)\n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder lstm_out size - {lstm_out.shape}\")\n",
    "        output = self.linear(lstm_out) \n",
    "        if DEBUG_FLAG:\n",
    "            print(f\"Decoder output size - {output.shape}\")\n",
    "        # output size: (1, batch_size, vocab_size)\n",
    "        \n",
    "#         output = output.squeeze(0)\n",
    "#         print(f\"Decoder output size - {output.shape}\")\n",
    "        # output size: (batch_size, vocab_size)\n",
    "        \n",
    "        return output, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "723ed77a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683646952924,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "723ed77a",
    "outputId": "190f6266-22c5-40fd-9628-3a65d6811e9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6304, 5938]),\n",
       " torch.Size([2, 6304, 30]),\n",
       " torch.Size([2, 6304, 30]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = bilstm_decoder(vocab_size, 30, 20, vocab_size)\n",
    "out, hidden_state = dec.forward(Y_train[0], enc_hidden_state)\n",
    "out.shape, hidden_state[0].shape, hidden_state[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b39d0dd7",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1683646953118,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "b39d0dd7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e705f38",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1683646953244,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "5e705f38"
   },
   "outputs": [],
   "source": [
    "class lstm_seq2seq(nn.Module):\n",
    "    ''' train LSTM encoder-decoder and make predictions '''\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of expected features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        '''\n",
    "\n",
    "        super(lstm_seq2seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def forward(self, source, target, target_vocab_size, teacher_force_ratio = 0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        \n",
    "        # encoder outputs\n",
    "        encoder_output, encoder_hidden = self.encoder.forward(source)\n",
    "        \n",
    "        # Grab start token\n",
    "        x = target[0]\n",
    "#         print(f\"seq2seq x size - {x.shape}\")\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            # decoder outputs\n",
    "            decoder_output, decoder_hidden = self.decoder(x, encoder_hidden)\n",
    "            \n",
    "            outputs[t] = decoder_output\n",
    "            # output size: (N, vocab_size)\n",
    "            \n",
    "            if DEBUG_FLAG: print(f\"seq2seq decoder_output size - {decoder_output.shape}\")\n",
    "            best_guess = decoder_output.argmax(1)\n",
    "            \n",
    "            if DEBUG_FLAG: print(f\"seq2seq best_guess size - {best_guess.shape} - {best_guess}\")\n",
    "            \n",
    "            if DEBUG_FLAG: print(f\"seq2seq target size - {target[t].shape} - {target[t]}\")\n",
    "            \n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, source, target_len, target_vocab_size, sos_idx):\n",
    "        \n",
    "        target_len = target_len+2\n",
    "        batch_size = source.shape[1]\n",
    "\n",
    "#         outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        outputs = torch.zeros(target_len, batch_size).to(device)\n",
    "\n",
    "        # encoder outputs\n",
    "        encoder_output, encoder_hidden = self.encoder.forward(source)\n",
    "\n",
    "        # Grab start token\n",
    "        x = torch.from_numpy(np.array([sos_idx]*batch_size))\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            \n",
    "            # decoder outputs\n",
    "            decoder_output, decoder_hidden = self.decoder(x, encoder_hidden)\n",
    "\n",
    "            # outputs[t] = decoder_output\n",
    "            # output size: (N, vocab_size)\n",
    "\n",
    "            best_guess = decoder_output.argmax(1)\n",
    "            x = best_guess\n",
    "            outputs[t] = best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4c46aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ckpt(dev_input_tensor, dev_target_tensor, model, criterion, epoch, batch_size, output_size):\n",
    "    \n",
    "    # calculate number of batch iterations\n",
    "    n_batches = int(dev_input_tensor.shape[1] / batch_size)\n",
    "#     print(f\"Number of batches - {n_batches}\")\n",
    "    \n",
    "    batch_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    model2.eval()\n",
    "    with trange(n_batches) as tr:\n",
    "        for b in tr:\n",
    "          # select data \n",
    "          inp_data = dev_input_tensor[:, b*batch_size : (b+1)*batch_size] #, :]\n",
    "          target = dev_target_tensor[:, b*batch_size : (b+1)*batch_size] #, :]\n",
    "\n",
    "          if torch.cuda.is_available():\n",
    "              inp_data, target = inp_data.cuda(), target.cuda()\n",
    "\n",
    "          output = model.forward(inp_data, target, output_size)\n",
    "\n",
    "          output = output[1:].reshape(-1, output.shape[2])\n",
    "          target = target[1:].reshape(-1)\n",
    "\n",
    "          if torch.cuda.is_available():\n",
    "              output = output.cuda()\n",
    "\n",
    "          # compute the loss\n",
    "          loss = criterion(output, target)\n",
    "          batch_loss += loss.item()\n",
    "\n",
    "          batch_count += 1\n",
    "                \n",
    "    # loss for epoch \n",
    "    batch_loss /= n_batches\n",
    "            \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd08025a",
   "metadata": {
    "executionInfo": {
     "elapsed": 124,
     "status": "ok",
     "timestamp": 1683647917913,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "cd08025a"
   },
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, dev_input_tensor, dev_target_tensor,\n",
    "          emb_size, hidden_size, vocab_size,\n",
    "          load_model = False, num_epochs = 2, lr = 0.0005, batch_size = 5):\n",
    "  \n",
    "    sub_folder_name = f\"baseline_lr{lr}_bs{batch_size}_es{emb_size}_hs{hidden_size}\"\n",
    "    models_directory = f\"models/{sub_folder_name}\"\n",
    "\n",
    "    if not os.path.isdir(models_directory):\n",
    "      os.makedirs(models_directory)\n",
    "    \n",
    "    input_size_encoder = vocab_size             # german\n",
    "    input_size_decoder = vocab_size             # english\n",
    "    output_size = vocab_size                    # english\n",
    "\n",
    "    encoder_embedding_size = emb_size\n",
    "    decoder_embedding_size = emb_size\n",
    "\n",
    "    # TENSORBOARD\n",
    "    writer = SummaryWriter(f'tb/loss_plot/{sub_folder_name}')\n",
    "    step = 0\n",
    "\n",
    "    encoder_net = bilstm_encoder(input_size_encoder, hidden_size, emb_size).to(device)\n",
    "    decoder_net = bilstm_decoder(input_size_decoder, hidden_size, emb_size, output_size).to(device)\n",
    "    \n",
    "    model = lstm_seq2seq(encoder_net, decoder_net).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    \n",
    "#     if load_model: load_checkpoint(torch.load('my_checkpoint.pth.ptar'), model, optimizer)\n",
    "        \n",
    "    # calculate number of batch iterations\n",
    "    n_batches = int(input_tensor.shape[1] / batch_size)\n",
    "    print(f\"Number of batches - {n_batches}\")\n",
    "    \n",
    "    # initialize array of losses \n",
    "    losses = np.full(num_epochs, np.nan)\n",
    "    losses_v2 = {'train': {}, \"val\": {}}\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "\n",
    "    #         checkpoint = {'state_dict': model.state_dict(),\n",
    "    #                       'optimizer': optimizer.state_dict()}\n",
    "    #         save_checkpoint(checkpoint)\n",
    "\n",
    "        batch_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        with trange(n_batches) as tr:\n",
    "            for b in tr:\n",
    "        \n",
    "              # select data \n",
    "              inp_data = input_tensor[:, b*batch_size : (b+1)*batch_size] #, :]\n",
    "              target = target_tensor[:, b*batch_size : (b+1)*batch_size] #, :]\n",
    "\n",
    "              if torch.cuda.is_available():\n",
    "                  inp_data, target = inp_data.cuda(), target.cuda()\n",
    "              \n",
    "  #             if step < 1:\n",
    "  #                 print(f\"batch_size - {b*batch_size} - {(b+1)*batch_size}\")\n",
    "  #                 print(f\"inp_data 0 - {inp_data}\")\n",
    "\n",
    "  #                 if step < 2:\n",
    "  #                     print(f\"inp_data 0 - {inp_data.shape}\")\n",
    "  #                     print(f\"target 0 - {target.shape}\")\n",
    "\n",
    "              output = model.forward(inp_data, target, output_size)\n",
    "              # output shape: (target_len, batch_size, output_dim)\n",
    "\n",
    "  #             if step < 1:\n",
    "  #                 print(f\"output size before reshape - {output.shape}\")\n",
    "  #                 print(f\"target size before reshape - {target.shape}\")\n",
    "\n",
    "              output = output[1:].reshape(-1, output.shape[2])\n",
    "              target = target[1:].reshape(-1)\n",
    "\n",
    "              if torch.cuda.is_available():\n",
    "                  output = output.cuda()\n",
    "\n",
    "  #             if step < 1:\n",
    "  #                 print(f\"output size after reshape - {output.shape}\")\n",
    "  #                 print(f\"target size after reshape - {target.shape}\")\n",
    "\n",
    "  #                 output = output.argmax(2)\n",
    "  #                 if step < 2:\n",
    "  #                     print(f\"output 3 - {output.shape} - {type(output)} - {output[:5]}\")\n",
    "  #                     print(f\"target 3 - {target.shape} - {type(target)} - {target[:5]}\")\n",
    "\n",
    "              # zero the gradient\n",
    "              optimizer.zero_grad()\n",
    "\n",
    "              # compute the loss\n",
    "              loss = criterion(output, target)\n",
    "              batch_loss += loss.item()\n",
    "\n",
    "              # backpropagation\n",
    "              loss.backward()\n",
    "              torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # for healthy gradients\n",
    "              optimizer.step()\n",
    "\n",
    "              writer.add_scalar('Training loss', loss, global_step=step)\n",
    "              step += 1\n",
    "\n",
    "              batch_count += 1\n",
    "              acc_batch_loss = batch_loss/batch_count\n",
    "            \n",
    "              # progress bar \n",
    "              tr.set_postfix({\"epoch_num\":epoch,\n",
    "                              \"loss\":f\"{acc_batch_loss:.3f}\"})\n",
    "        \n",
    "        # loss for epoch \n",
    "        batch_loss /= n_batches \n",
    "        losses[epoch-1] = batch_loss\n",
    "        losses_v2['train'][epoch] = batch_loss\n",
    "        \n",
    "        val_loss = eval_ckpt(dev_input_tensor, dev_target_tensor, model, criterion,\n",
    "                            epoch, batch_size, output_size)\n",
    "        losses_v2['val'][epoch] = val_loss\n",
    "        print(f\"val loss {val_loss}\")\n",
    "        with open(os.path.join(models_directory, \"loss.json\"), 'w') as f:\n",
    "          json.dump(losses_v2, f)\n",
    "            \n",
    "        # save models\n",
    "        if (epoch > 4 and epoch % 2 == 0):\n",
    "            torch.save(model, os.path.join(models_directory, f\"model_{epoch}\"))\n",
    "        # break\n",
    "\n",
    "    torch.save(model, os.path.join(models_directory, f\"model_last_{epoch}\"))\n",
    "            \n",
    "    return losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1fa70ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4738891,
     "status": "ok",
     "timestamp": 1683652659212,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "c1fa70ea",
    "outputId": "baab219e-a7fb-47f1-dc1b-4b23057fe701",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches - 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.96it/s, epoch_num=1, loss=8.800]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 42.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 8.739952850341798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.14it/s, epoch_num=2, loss=8.780]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 42.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss 8.731201171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss, model = train(X_train, Y_train, X_val, Y_val, emb_size=20, hidden_size=10, vocab_size=vocab_size,\n",
    "            num_epochs = 2, lr = 0.001, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb8769fb",
   "metadata": {
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1683652853853,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "bb8769fb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb23767e",
   "metadata": {
    "id": "eb23767e"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3880e833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aishwarya/Downloads/spring23/cs685-NLP/project/notebooks'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b84aa4",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "626d0704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encoder input - (43, 5)\n",
      "Train decoder input - (129, 5)\n",
      "{'vocab_size': 5938, 'max_encoder_len': 43, 'max_decoder_len': 127, 'pad_idx': 1462, 'sos_idx': 1463, 'eos_idx': 1461}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([43, 5]), torch.Size([129, 5]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# local\n",
    "# target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/LSTM_encoder_decoder/data/data_final_processed_v2\"\n",
    "# target_folder = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/baseline/training_data\"\n",
    "\n",
    "# for colab\n",
    "target_folder = \"../data/baseline/training_data\"\n",
    "\n",
    "fol1 = 'test'\n",
    "\n",
    "## GET DATA\n",
    "#sample data for checking network\n",
    "data_t = 'encode'\n",
    "X_val_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "X_val_np = X_val_np[:5]\n",
    "X_val_np = X_val_np.transpose(1,0)\n",
    "# train_input = np.expand_dims(train_input, axis=-1) \n",
    "print(f'Train encoder input - {X_val_np.shape}')\n",
    "\n",
    "\n",
    "#sample data for checking network\n",
    "data_t = 'decode'\n",
    "Y_val_np = np.load(os.path.join(target_folder, fol1, f\"{fol1}_{data_t}.npy\"))\n",
    "Y_val_np = Y_val_np[:5]\n",
    "Y_val_np = Y_val_np.transpose(1,0)\n",
    "# train_output = np.expand_dims(train_output, axis=-1) \n",
    "print(f'Train decoder input - {Y_val_np.shape}')\n",
    "\n",
    "with open(os.path.join(target_folder, 'data_info.json'), 'r') as fp:\n",
    "    data_info = json.load(fp)\n",
    "    \n",
    "pad_idx = data_info['pad_idx']\n",
    "sos_idx = data_info['sos_idx']\n",
    "eos_idx = data_info['eos_idx']\n",
    "vocab_size = data_info['vocab_size']\n",
    "max_decoder_len = data_info[\"max_decoder_len\"]\n",
    "    \n",
    "print(data_info)\n",
    "\n",
    "# convert numpy array to tensors\n",
    "X_val = torch.from_numpy(X_val_np).type(torch.int64) #torch.int64, torch.Tensor\n",
    "Y_val = torch.from_numpy(Y_val_np).type(torch.int64)\n",
    "\n",
    "X_val.shape, Y_val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98b06136",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(target_folder, 'idx_to_vocab.json'), 'r') as fp:\n",
    "    idx_to_vocab = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096d182",
   "metadata": {},
   "source": [
    "## Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a394b72c",
   "metadata": {
    "id": "a394b72c",
    "outputId": "9b00eb8d-410b-4190-86fe-23c8f6ccc8da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_seq2seq(\n",
       "  (encoder): bilstm_encoder(\n",
       "    (embeddings): Embedding(5938, 200)\n",
       "    (lstm): LSTM(200, 100, bidirectional=True)\n",
       "  )\n",
       "  (decoder): bilstm_decoder(\n",
       "    (embeddings): Embedding(5938, 200)\n",
       "    (lstm): LSTM(200, 100, num_layers=2)\n",
       "    (linear): Linear(in_features=100, out_features=5938, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model class must be defined somewhere\n",
    "model_path = os.path.join(os.getcwd(), \"models/baseline_lr0.001_bs64_es200_hs100/model_last_100\")\n",
    "\n",
    "model2 = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32516187",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model2.predict(X_val, max_decoder_len, vocab_size, sos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e71e65a",
   "metadata": {
    "id": "4e71e65a",
    "outputId": "01689147-98d9-48ba-9a1f-ff1869b4bad0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([43, 696]), torch.Size([129, 696]), torch.Size([129, 696]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape, Y_val.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6df262c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_mat = []\n",
    "for c in range(outputs.shape[1]):\n",
    "    \n",
    "    check = outputs[:,c].numpy()\n",
    "    col = []\n",
    "    for x, value in np.ndenumerate(check):\n",
    "\n",
    "        value_2 = int(value)\n",
    "        \n",
    "        # skip is sos\n",
    "        if x[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        # exit loop if eos\n",
    "        if value_2 == eos_idx:\n",
    "            break\n",
    "        \n",
    "        text = idx_to_vocab[str(value_2)]\n",
    "        col.append(text)\n",
    "   \n",
    "    output_mat.append(col)\n",
    "\n",
    "len(output_mat), output_mat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c3af05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696,\n",
       " ['select',\n",
       "  'count',\n",
       "  '(',\n",
       "  '*',\n",
       "  ')',\n",
       "  ',',\n",
       "  'competition',\n",
       "  'from',\n",
       "  'match',\n",
       "  'group',\n",
       "  'by',\n",
       "  'competition'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_mat = []\n",
    "for c in range(Y_val.shape[1]):\n",
    "    \n",
    "    check = Y_val[:,c].numpy()\n",
    "    col = []\n",
    "    for x, value in np.ndenumerate(check):\n",
    "\n",
    "        value_2 = int(value)\n",
    "        \n",
    "        # skip is sos\n",
    "        if x[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        # exit loop if eos\n",
    "        if value_2 == eos_idx:\n",
    "            break\n",
    "        \n",
    "        text = idx_to_vocab[str(value_2)]\n",
    "        col.append(text)\n",
    "   \n",
    "    gt_mat.append(col)\n",
    "\n",
    "len(gt_mat), gt_mat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5898561a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['select',\n",
       "  't2',\n",
       "  '.',\n",
       "  'name',\n",
       "  'from',\n",
       "  'assignedto',\n",
       "  'as',\n",
       "  't1',\n",
       "  'join',\n",
       "  'scientists',\n",
       "  'as',\n",
       "  't2',\n",
       "  'on',\n",
       "  't1',\n",
       "  '.',\n",
       "  'scientist',\n",
       "  '=',\n",
       "  't2',\n",
       "  '.',\n",
       "  'ssn'],\n",
       " ['select',\n",
       "  't2',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1',\n",
       "  '.',\n",
       "  't1'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idy = 200\n",
    "gt_mat[idy], output_mat[idy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38517662",
   "metadata": {
    "id": "38517662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select count ( * ) from documents\n",
      "select count ( * ) from documents\n",
      "True\n",
      "\n",
      "select count ( * ) from train\n",
      "select count ( * ) from train\n",
      "True\n",
      "\n",
      "select count ( * ) from staff\n",
      "select count ( * ) from staff\n",
      "True\n",
      "\n",
      "select count ( * ) from customers\n",
      "select count ( * ) from customers\n",
      "True\n",
      "\n",
      "select count ( * ) from customers\n",
      "select count ( * ) from customers\n",
      "True\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 0.007183908045977011)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_em = 0\n",
    "for i in range(len(gt_mat)):\n",
    "    \n",
    "    gt = ' '.join(gt_mat[i])\n",
    "    pred = ' '.join(output_mat[i])\n",
    "    \n",
    "    if gt == pred:\n",
    "        count_em += 1\n",
    "        print(gt)\n",
    "        print(pred)\n",
    "        print(gt == pred)\n",
    "        print()\n",
    "        \n",
    "count_em, count_em/len(gt_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab5fb8",
   "metadata": {
    "id": "46ab5fb8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
