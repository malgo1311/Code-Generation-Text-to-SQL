{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc49d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22434,
     "status": "ok",
     "timestamp": 1683756039554,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "44dc49d1",
    "outputId": "f71e642f-cc11-4720-847d-9983cfafdaad"
   },
   "outputs": [],
   "source": [
    "# # MOUNTING GOOGLE DRIVE\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import os\n",
    "# print(os.getcwd())\n",
    "\n",
    "# wd = '/content/drive/MyDrive/CS 685/cs685_project/notebooks'\n",
    "# print(os.listdir(wd))\n",
    "# os.chdir(wd)\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EOrjkzvAqCng",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28261,
     "status": "ok",
     "timestamp": 1683756067809,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "EOrjkzvAqCng",
    "outputId": "a12095b7-7146-43c2-e55b-2153180dfe64",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tokenizers\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee9ce01e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13533,
     "status": "ok",
     "timestamp": 1683756081335,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "ee9ce01e",
    "outputId": "754fbbbc-1c71-4490-81a9-e08292caace6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "from load_dataset import Text2SQLDataset, Text2SQLDataset_schema\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tokenizers import AddedToken\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "from transformers import BertModel, T5ForConditionalGeneration, T5Tokenizer, BertTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc97230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bert_t5_net import EncoderDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc843bcd",
   "metadata": {
    "id": "bc843bcd"
   },
   "source": [
    "## Need to replace BERT as GNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17abad9d",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1683756081335,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "17abad9d"
   },
   "outputs": [],
   "source": [
    "# FOR PRINTING INTERMEDIATE TORCH SIZES\n",
    "DEBUG_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4921e160",
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1683756383738,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "4921e160"
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, bert_hidden_size, t5_hidden_size, max_input_length, \n",
    "                 max_output_length, bert_model, t5_model, batch_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "        self.t5_hidden_size = t5_hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(bert_model)\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(t5_model)\n",
    "#         self.linear = nn.Linear(bert_hidden_size, t5_hidden_size)\n",
    "        self.linear = nn.Linear(bert_hidden_size, max_input_length*t5_hidden_size)\n",
    "    \n",
    "        self.t5.config.is_encoder_decoder = False\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids, input_mask,\n",
    "                decoder_input_ids, decoder_attention_mask):\n",
    "        \n",
    "        # Encode input with BERT\n",
    "        _, bert_output = self.bert(input_ids=input_ids, attention_mask=input_mask, return_dict=False)\n",
    "        \n",
    "        if DEBUG_FLAG: print(f\"bert_output - {bert_output.size()}\")\n",
    "        \n",
    "        # Transform BERT output to T5 input shape\n",
    "        t5_input = self.linear(bert_output)\n",
    "        if DEBUG_FLAG: print(f\"bert_output linear - {t5_input.size()}\")\n",
    "        \n",
    "#         t5_input = t5_input.unsqueeze(1).repeat(1, self.max_output_length, 1)\n",
    "#         if DEBUG_FLAG: print(f\"bert_output linear unsqueeze - {t5_input.size()}\")\n",
    "        \n",
    "        t5_input = t5_input.view(self.batch_size, self.max_input_length, self.t5_hidden_size)\n",
    "        t5_input = t5_input.unsqueeze(0)\n",
    "        if DEBUG_FLAG: print(f\"t5_input - {t5_input.size()}\")\n",
    "        \n",
    "#         t5_outputs = self.t5(decoder_input_ids=decoder_input_ids,\n",
    "#                              decoder_attention_mask=decoder_attention_mask,\n",
    "#                              encoder_outputs=t5_input\n",
    "#                             )\n",
    "#         if DEBUG_FLAG: print(f\"t5_input logits - {(t5_outputs.logits).size()}\")\n",
    "#         return t5_outputs.logits\n",
    "    \n",
    "        t5_outputs = self.t5(labels=decoder_input_ids,\n",
    "                             decoder_attention_mask=decoder_attention_mask,\n",
    "                             encoder_outputs=t5_input,\n",
    "                             return_dict = True\n",
    "                            )\n",
    "        \n",
    "        if DEBUG_FLAG: print(f\"t5_input - {type(t5_outputs)}\")\n",
    "        return t5_outputs\n",
    "\n",
    "    def predict(self, input_ids, input_mask, batch_size, t5_tokenizer):\n",
    "        \n",
    "        self.bert.eval()\n",
    "        _, bert_output = self.bert(input_ids=input_ids, attention_mask=input_mask, return_dict=False)\n",
    "        if DEBUG_FLAG: print(f\"bert_output - {bert_output.size()}\")\n",
    "        \n",
    "        # Transform BERT output to T5 input shape\n",
    "        t5_input = self.linear(bert_output)\n",
    "        if DEBUG_FLAG: print(f\"t5_input - {t5_input.size()} - {t5_input}\")\n",
    "        t5_input = t5_input.view(batch_size, self.max_input_length, self.t5_hidden_size)\n",
    "        t5_input = t5_input.unsqueeze(0)\n",
    "        if DEBUG_FLAG: print(f\"t5_input - {t5_input.size()} - {t5_input}\")\n",
    "        \n",
    "        # Generate initial input for T5 decoder\n",
    "        start_token = t5_tokenizer.pad_token_id\n",
    "        \n",
    "#         decoder_input_ids = torch.tensor([start_token] * batch_size).unsqueeze(0)\n",
    "#         decoder_attention_mask = torch.tensor([1] * batch_size).unsqueeze(0)\n",
    "        \n",
    "        decoder_input_ids = torch.tensor([start_token]*batch_size).unsqueeze(0)\n",
    "        decoder_attention_mask = torch.tensor([1]*batch_size).unsqueeze(0)\n",
    "    \n",
    "#         decoder_input_ids = decoder_input_ids.view(decoder_input_ids.shape[1],\n",
    "#                                                    decoder_input_ids.shape[0])\n",
    "#         decoder_attention_mask = decoder_attention_mask.view(decoder_attention_mask.shape[1],\n",
    "#                                                              decoder_attention_mask.shape[0])\n",
    "        \n",
    "        print(f\"decoder_input_ids - {decoder_input_ids.size()}\")\n",
    "        print(f\"decoder_attention_mask - {decoder_attention_mask.size()}\")\n",
    "        \n",
    "        print(f\"initial decoder_input_ids - {decoder_input_ids}\")\n",
    "        \n",
    "        # Use the model to get output logits\n",
    "        # Predict the output\n",
    "        self.t5.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(50):  # Maximum length of generated sequence\n",
    "                t5_outputs = self.t5(decoder_input_ids=decoder_input_ids,\n",
    "                                     decoder_attention_mask=decoder_attention_mask,\n",
    "                                     encoder_outputs=t5_input)\n",
    "#                 print(f\"t5_outputs - {t5_outputs}\")\n",
    "                print(f\"t5_outputs logits - {(t5_outputs.logits).size()}\")\n",
    "    \n",
    "                next_token_logits = t5_outputs.logits[:, -1, :]\n",
    "                print(f\"next_token_logits - {next_token_logits.size()}\")\n",
    "            \n",
    "#                 next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "                next_token_id = next_token_logits.argmax(1)\n",
    "#                 print(f\"next_token_id - {next_token_id.size()}\")\n",
    "#                 print(f\"next_token_id.unsqueeze(-1) - {next_token_id.unsqueeze(-1).size()}\")\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
    "                decoder_attention_mask = torch.cat([decoder_attention_mask,\n",
    "                                                    torch.ones_like(next_token_id.unsqueeze(-1))], dim=-1)\n",
    "\n",
    "                if next_token_id == t5_tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                print(f\"pred decoder_input_ids - {decoder_input_ids}\")\n",
    "                \n",
    "#                 break\n",
    "        \n",
    "        # generated_text\n",
    "#         t5_outputs = t5_tokenizer.decode(decoder_input_ids.squeeze(), skip_special_tokens=True)\n",
    "        t5_outputs = decoder_input_ids #.squeeze()\n",
    "        \n",
    "        return t5_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e519d2d4",
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1683756664817,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "e519d2d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(train_filepath, train_schema_filepath, batch_size, bert_hidden_size, t5_hidden_size, lr, num_epochs,\n",
    "         max_input_length, max_output_length, bert_model, t5_model):\n",
    "    \n",
    "    sub_folder_name = f\"BERT_T5_lr{lr}_bs{batch_size}_{bert_model}_{t5_model}\"\n",
    "    models_directory = f\"models/{sub_folder_name}\"\n",
    "\n",
    "    if not os.path.isdir(models_directory):\n",
    "        os.makedirs(models_directory)\n",
    "        \n",
    "    # TENSORBOARD\n",
    "    writer = SummaryWriter(f'tb/loss_plot/{sub_folder_name}')\n",
    "    \n",
    "    train_dataset = Text2SQLDataset_schema(\n",
    "            dir_ = train_filepath,\n",
    "            schema_dir_ = train_schema_filepath,\n",
    "            mode = \"train\")\n",
    "\n",
    "    train_dataloder = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size = batch_size, \n",
    "            shuffle = True,\n",
    "            collate_fn = lambda x: x,\n",
    "            drop_last = True\n",
    "        )\n",
    "    \n",
    "    print(f\"Number of batches - {len(train_dataloder)}\")\n",
    "\n",
    "    # Define BERT and T5 tokenizers\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
    "    print(f\"Tokenizers loaded\")\n",
    "\n",
    "    model = EncoderDecoder(bert_hidden_size, t5_hidden_size, \n",
    "                           max_input_length, max_output_length,\n",
    "                           bert_model, t5_model, batch_size).to(device)\n",
    "    print(f\"Model loaded\")\n",
    "#     print(f\"{model.config.decoder_start_token_id}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    print(f\"Otimizer - Adam\")\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss(ignore_index=t5_tokenizer.pad_token_id)\n",
    "#     criterion = ContrastiveLoss()\n",
    "    print(f\"CrossEntropyLoss initialized\")\n",
    "    \n",
    "    # initialize array of losses \n",
    "    losses = {'train': {}, \"val\": {}}\n",
    "\n",
    "    # for epoch in range(num_epochs):\n",
    "    with trange(num_epochs) as tr:\n",
    "        for epoch in tr:\n",
    "            \n",
    "            # Train the model\n",
    "            model.train()\n",
    "            \n",
    "            batch_loss = 0\n",
    "            \n",
    "            for idx, batch in enumerate(train_dataloder):\n",
    "\n",
    "                batch_inputs = [data[0] for data in batch]\n",
    "                batch_sqls = [data[1] for data in batch]\n",
    "\n",
    "                if DEBUG_FLAG:\n",
    "                    if epoch == 0 and idx == 0:\n",
    "                        print(f\"batch_inputs - {type(batch_inputs)} {len(batch_inputs)}\")\n",
    "                        print(f\"batch_sqls - {type(batch_sqls)} {len(batch_sqls)}\")\n",
    "                        \n",
    "                for temp_i, temp in enumerate(batch_inputs):\n",
    "                    print(f\"batch_inputs - {batch_inputs[temp_i]}\")\n",
    "                    print(f\"batch_sqls - {batch_sqls[temp_i]}\")\n",
    "\n",
    "                tokenized_inputs = bert_tokenizer(batch_inputs,\n",
    "                                                  add_special_tokens=True,\n",
    "                                                  padding=\"max_length\", #True,\n",
    "                                                  max_length=max_input_length,\n",
    "                                                  #pad_to_max_length=True,\n",
    "                                                  return_tensors='pt',\n",
    "                                                  truncation=True)\n",
    "\n",
    "                encoder_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
    "                encoder_input_attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "#                 print(f\"encoder_input_ids - {encoder_input_ids}\")\n",
    "                tokenized_outputs = t5_tokenizer(batch_sqls,\n",
    "                                                 add_special_tokens=True,\n",
    "                                                 padding=\"max_length\", #True,\n",
    "                                                 max_length=max_output_length,\n",
    "                                                 #pad_to_max_length=True,\n",
    "                                                 return_tensors='pt',\n",
    "                                                 truncation=True)\n",
    "\n",
    "\n",
    "                decoder_input_ids = tokenized_outputs[\"input_ids\"].to(device)\n",
    "                # replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "#                 decoder_input_ids[decoder_input_ids == t5_tokenizer.pad_token_id] = -100\n",
    "                decoder_attention_mask = tokenized_outputs[\"attention_mask\"].to(device)\n",
    "#                 labels = None #tokenized_outputs[\"attention_mask\"].to(device)\n",
    "\n",
    "#                 print(f\"decoder_input_ids - {decoder_input_ids}\")\n",
    "\n",
    "                if DEBUG_FLAG and epoch == 0 and idx == 0:\n",
    "                    print(f\"encoder_input_ids - {encoder_input_ids.size()}\")\n",
    "                    print(f\"encoder_input_attention_mask - {encoder_input_attention_mask.size()}\")\n",
    "                    print(f\"decoder_input_ids - {decoder_input_ids.size()}\")\n",
    "                    print(f\"decoder_attention_mask - {decoder_attention_mask.size()}\")\n",
    "\n",
    "                # Clear gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                model_output = model(encoder_input_ids,\n",
    "                               encoder_input_attention_mask,\n",
    "                               decoder_input_ids,\n",
    "                               decoder_attention_mask)\n",
    "#                                labels=labels)\n",
    "\n",
    "                print(f\"model_output - {type(model_output)}\")\n",
    "#                 print(f\"model_output - {(model_output)}\")\n",
    "                \n",
    "                output = model_output[\"logits\"]\n",
    "#                 print(f\"output - {output.size()}\")\n",
    "#                 print(f\"decoder_input_ids - {decoder_input_ids.size()}\")\n",
    "                \n",
    "                output_resize = output.view(output.shape[0]*output.shape[1], output.shape[2])\n",
    "                decoder_input_ids_resize = decoder_input_ids.view(decoder_input_ids.shape[0]*decoder_input_ids.shape[1])\n",
    "                \n",
    "#                 print(f\"output_resize - {output_resize.size()}\")\n",
    "#                 print(f\"decoder_input_ids_resize - {decoder_input_ids_resize.size()}\")\n",
    "                \n",
    "#                 print(f\"output - {model_output}\")\n",
    "                \n",
    "#                 actual = decoder_input_ids_resize\n",
    "#                 predicted_classes = torch.argmax(output_resize, dim=-1)\n",
    "#                 print(f\"predicted_classes - {predicted_classes.size()}\")\n",
    "# #                 print(f\"decoder_input_ids_resize - {decoder_input_ids_resize}\")\n",
    "#                 loss = criterion(actual, predicted_classes)\n",
    "#                 print(f\"loss - {loss}\")\n",
    "            \n",
    "#                 batch_loss += loss.item()\n",
    "\n",
    "                loss = model_output[\"loss\"]\n",
    "                batch_loss += loss\n",
    "            \n",
    "                # backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                break\n",
    "                \n",
    "            batch_loss /= len(train_dataloder) \n",
    "            losses['train'][epoch] = f\"{batch_loss:.3f}\"\n",
    "            #progress bar \n",
    "            tr.set_postfix({\"epoch_num\":epoch,\n",
    "                            \"loss\":f\"{batch_loss:.10f}\"})\n",
    "            \n",
    "#             with open(os.path.join(models_directory, \"loss.json\"), 'w') as f:\n",
    "#                 json.dump(losses, f)\n",
    "            \n",
    "#             writer.add_scalar('Training loss', batch_loss, global_step=epoch+1)\n",
    "#             # save models\n",
    "#             if (epoch > 3 and epoch % 5 == 0):\n",
    "#                 torch.save(model, os.path.join(models_directory, f\"model_{epoch}\"))\n",
    "#     torch.save(model, os.path.join(models_directory, f\"model_last_{epoch}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4444e046",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420541,
     "status": "ok",
     "timestamp": 1683757085355,
     "user": {
      "displayName": "Aishwarya Malgonde",
      "userId": "02733017902828562032"
     },
     "user_tz": 240
    },
    "id": "4444e046",
    "outputId": "279d8873-1753-4cf1-9622-eaab0b82a39a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of schema_dataset 6266\n",
      "Key not found - Count the number of services.\n",
      "Number of batches - 3151\n",
      "Tokenizers loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Otimizer - Adam\n",
      "CrossEntropyLoss initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                 | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_inputs - <class 'list'> 2\n",
      "batch_sqls - <class 'list'> 2\n",
      "batch_inputs - find the id and last name of the student that has the most behavior incidents ? | teachers : teacher id ( [FK] ) , middle name , other details , email address , last name , first name , address id ( [FK] ) , gender , cell mobile number | addresses : other address details , zip postcode , state province county , country , line 3 , line 1 , city , line 2 , address id ( [FK] ) | behavior incident : other details , date incident end , student id ( [FK] ) , incident type code ( [FK] ) , date incident start , recommendations , incident summary , incident id ( [FK] ) | detention : teacher id ( [FK] ) , detention summary , detention type code ( [FK] ) , detention id ( [FK] ) , other details , datetime detention start , datetime detention end | students : last name , address id ( [FK] ) , student id ( [FK] ) , other student details , date left university , middle name , first name , cell mobile number , email address , date first rental | student addresses : date address to , student id ( [FK] ) , date address from , address id ( [FK] ) , other details , monthly rental | assessment notes : text of notes , student id ( [FK] ) , notes id , teacher id ( [FK] ) , other details , date of notes | reference incident type : incident type description , incident type code ( [FK] ) | reference detention type : detention type description , detention type code ( [FK] ) | reference address types : address type description , address type code | students in detention : incident id ( [FK] ) , detention id ( [FK] ) , student id ( [FK] )\n",
      "batch_sqls - select _ from _ group by _ order by count ( _ ) desc limit _ | select behavior_incident.student_id , students.last_name from behavior_incident join students on behavior_incident.student_id = students.student_id group by behavior_incident.student_id order by count ( * ) desc limit 1\n",
      "batch_inputs - show the average amount of transactions for different investors .  | sales : sales transaction id ( [FK] ) , sales details | purchases : purchase transaction id ( [FK] ) , purchase details | transactions : transaction type code ( [FK] ) , amount of transaction , transaction id ( [FK] ) , share count , other details , investor id ( [FK] ) , date of transaction | lots : lot details , lot id ( [FK] ) , investor id ( [FK] ) | investors : investor details , investor id ( [FK] ) | transactions lots : transaction id ( [FK] ) , lot id ( [FK] ) | reference transaction types : transaction type description , transaction type code ( [FK] )\n",
      "batch_sqls - select _ , avg ( _ ) from _ group by _ | select investor_id , avg ( amount_of_transaction ) from transactions group by investor_id\n",
      "encoder_input_ids - torch.Size([2, 512])\n",
      "encoder_input_attention_mask - torch.Size([2, 512])\n",
      "decoder_input_ids - torch.Size([2, 128])\n",
      "decoder_attention_mask - torch.Size([2, 128])\n",
      "bert_output - torch.Size([2, 768])\n",
      "bert_output linear - torch.Size([2, 262144])\n",
      "t5_input - torch.Size([1, 2, 512, 512])\n",
      "t5_input - <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
      "model_output - <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.17s/it, epoch_num=0, loss=0.0042633787]\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "train(train_filepath = \"../data/resdsql_pre/preprocessed_dataset_train.json\",\n",
    "      train_schema_filepath = \"../data/resdsql_pre/preprocessed_dataset_train_db_schema.json\",\n",
    "      batch_size = 2, #32\n",
    "      bert_hidden_size = 768,\n",
    "      t5_hidden_size = 512,\n",
    "      lr = 1e-4,\n",
    "      num_epochs = 1, #300\n",
    "      max_input_length = 512, #43,\n",
    "      max_output_length = 128, #127,\n",
    "      bert_model = 'bert-base-uncased',\n",
    "      t5_model = 't5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa55e8",
   "metadata": {
    "id": "bbaa55e8"
   },
   "outputs": [],
   "source": [
    "actual.size(), predicted_classes.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21797c26",
   "metadata": {
    "id": "083df0db"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b910f",
   "metadata": {
    "id": "6b6b910f"
   },
   "outputs": [],
   "source": [
    "# Model class must be defined somewhere\n",
    "\n",
    "model_folder = \"models/BERT_T5_lr0.0001_bs32_bert-base-uncased_t5-small_v3\"\n",
    "\n",
    "model_path = os.path.join(os.getcwd(), model_folder, \"model_40\")\n",
    "output = os.path.join(os.getcwd(), model_folder, \"model_40.txt\")\n",
    "\n",
    "model2 = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353361ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm\n",
    "from text2sql_decoding_utils import decode_sqls\n",
    "from spider_metric.evaluator import EvaluateTool\n",
    "\n",
    "dev_filepath = \"../data/resdsql_pre/preprocessed_dataset_test.json\"\n",
    "original_dev_filepath = \"../data/split/spider_test.json\"\n",
    "\n",
    "batch_size = 1\n",
    "max_input_length = 43\n",
    "bert_model = 'bert-base-uncased'\n",
    "t5_model = 't5-small'\n",
    "db_path = \"../spider_data/database\"\n",
    "mode = \"eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38bf753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# initialize tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
    "\n",
    "# if isinstance(tokenizer, T5TokenizerFast):\n",
    "#     tokenizer.add_tokens([AddedToken(\" <=\"), AddedToken(\" <\")])\n",
    "\n",
    "dev_dataset = Text2SQLDataset(\n",
    "            dir_ = dev_filepath,\n",
    "            mode = mode)\n",
    "\n",
    "dev_dataloder = DataLoader(\n",
    "        dev_dataset, \n",
    "        batch_size = batch_size, \n",
    "        shuffle = False,\n",
    "        collate_fn = lambda x: x,\n",
    "        drop_last = False\n",
    "    )\n",
    "\n",
    "# initialize model\n",
    "\n",
    "model2.eval()\n",
    "predict_sqls = []\n",
    "# for batch in tqdm(dev_dataloder):\n",
    "for idx, batch in enumerate(dev_dataloder):\n",
    "    batch_inputs = [data[0] for data in batch]\n",
    "    batch_db_ids = [data[1] for data in batch]\n",
    "    batch_tc_original = [data[2] for data in batch]\n",
    "\n",
    "    tokenized_inputs = bert_tokenizer(batch_inputs,\n",
    "                                      add_special_tokens=True,\n",
    "                                      padding=\"max_length\", #True,\n",
    "                                      max_length=max_input_length,\n",
    "                                      #pad_to_max_length=True,\n",
    "                                      return_tensors='pt',\n",
    "                                      truncation=True)\n",
    "\n",
    "    encoder_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
    "    encoder_input_attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    print(f\"encoder_input_ids - {encoder_input_ids.size()}\")\n",
    "    print(f\"encoder_input_attention_mask - {encoder_input_attention_mask.size()}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model2.predict(encoder_input_ids, encoder_input_attention_mask,\n",
    "                                       batch_size, t5_tokenizer=t5_tokenizer)\n",
    "\n",
    "        model_outputs = model_outputs.view(batch_size, 1, model_outputs.shape[1])\n",
    "        \n",
    "        predict_sqls += decode_sqls(\n",
    "                                    db_path, \n",
    "                                    model_outputs, \n",
    "                                    batch_db_ids, \n",
    "                                    batch_inputs, \n",
    "                                    t5_tokenizer, \n",
    "                                    batch_tc_original\n",
    "                                    )\n",
    "    break\n",
    "\n",
    "\n",
    "new_dir = \"/\".join(output.split(\"/\")[:-1]).strip()\n",
    "if new_dir != \"\":\n",
    "    os.makedirs(new_dir, exist_ok = True)\n",
    "\n",
    "# save results\n",
    "with open(output, \"w\", encoding = 'utf-8') as f:\n",
    "    for pred in predict_sqls:\n",
    "        f.write(pred + \"\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Text-to-SQL inference spends {}s.\".format(end_time-start_time))\n",
    "\n",
    "if mode == \"eval\":\n",
    "    # initialize evaluator\n",
    "    evaluator = EvaluateTool()\n",
    "    evaluator.register_golds(original_dev_filepath, db_path)\n",
    "    spider_metric_result = evaluator.evaluate(predict_sqls)\n",
    "    print('exact_match score: {}'.format(spider_metric_result[\"exact_match\"]))\n",
    "    print('exec score: {}'.format(spider_metric_result[\"exec\"]))\n",
    "    \n",
    "    em_res = spider_metric_result[\"exact_match\"]\n",
    "    ex_res = spider_metric_result[\"exec\"]\n",
    "#     return spider_metric_result[\"exact_match\"], spider_metric_result[\"exec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs, model_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf57136",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs, batch_db_ids, batch_tc_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e5efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
