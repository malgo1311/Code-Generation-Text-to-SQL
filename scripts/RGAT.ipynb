{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:13:50.103910400Z",
     "start_time": "2023-05-09T05:13:26.433261400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Atharva\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.1.0)/charset_normalizer (2.0.10) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertConfig, BertPreTrainedModel, BertTokenizer\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from model_gcn import GAT, GCN, Rel_GAT\n",
    "from model_utils import LinearAttention, DotprodAttention, RelationAttention, Highway, mask_logits\n",
    "from tree import *\n",
    "# used reference from : https://github.com/shenwzh3/RGAT-ABSA\n",
    "# inspired from https://aclanthology.org/2020.acl-main.295.pdf\n",
    "import os\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "'''\n",
    "Trying to add RGAT layers in initial graphix based approach\n",
    "\n",
    "Used some parts from natsql repo\n",
    "'''\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class R_GATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_relations, num_heads=1):\n",
    "        super(R_GATLayer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_relations = num_relations\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.W = nn.Parameter(torch.Tensor(num_heads, in_dim, out_dim))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2*out_dim, 1))\n",
    "        nn.init.xavier_uniform_(self.a)\n",
    "\n",
    "        self.relations = nn.Parameter(torch.Tensor(num_relations, out_dim, out_dim))\n",
    "        nn.init.xavier_uniform_(self.relations)\n",
    "\n",
    "    def forward(self, x, edge_lists):\n",
    "        h = torch.matmul(x, self.W)\n",
    "\n",
    "        heads = []\n",
    "        for i in range(self.num_heads):\n",
    "            attention_input = torch.cat([h[edge_lists[:, 0]], h[edge_lists[:, 1]], self.relations[edge_lists[:, 2]]], dim=-1)\n",
    "            attention_logits = torch.matmul(attention_input, self.a[i])\n",
    "            attention_weights = F.softmax(attention_logits, dim=0)\n",
    "            head = torch.sum(attention_weights * h[edge_lists[:, 1]], dim=0)\n",
    "            heads.append(head)\n",
    "\n",
    "        output = torch.mean(torch.stack(heads), dim=0)\n",
    "\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class R_GAT(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_relations, num_heads=1, num_layers=1):\n",
    "        super(R_GAT, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(R_GATLayer(in_dim, out_dim, num_relations, num_heads))\n",
    "            else:\n",
    "                self.layers.append(R_GATLayer(out_dim, out_dim, num_relations, num_heads))\n",
    "\n",
    "    def forward(self, x, edge_lists):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x, edge_lists)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Aspect_Text_GAT_only(nn.Module):\n",
    "    \"\"\"\n",
    "    reshape tree in GAT only\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings, embed_dim,dropout, highway, num_layers, embedding_dim, hidden_size, gat_attention_type, final_hidden_size, num_mlps, num_heads, num_classes ):\n",
    "        super(Aspect_Text_GAT_only, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings, embed_dim)\n",
    "        # self.embed.weight = nn.Parameter(args.glove_embedding, requires_grad=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        if highway:\n",
    "            self.highway = Highway(num_layers, embedding_dim)\n",
    "\n",
    "        self.bilstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size,\n",
    "                                  bidirectional=True, batch_first=True, num_layers=num_layers)\n",
    "        gcn_input_dim = hidden_size * 2\n",
    "\n",
    "        # if args.gat:\n",
    "        if gat_attention_type == 'linear':\n",
    "            self.gat = [LinearAttention(in_dim = gcn_input_dim, mem_dim = gcn_input_dim).to(args.device) for i in range(args.num_heads)] # we prefer to keep the dimension unchanged\n",
    "        elif gat_attention_type == 'dotprod':\n",
    "            self.gat = [DotprodAttention().to(device) for i in range(num_heads)]\n",
    "        else:\n",
    "            # reshaped gcn\n",
    "            self.gat = nn.Linear(gcn_input_dim, gcn_input_dim)\n",
    "\n",
    "\n",
    "        last_hidden_size = hidden_size * 2\n",
    "\n",
    "        layers = [\n",
    "            nn.Linear(last_hidden_size, final_hidden_size), nn.ReLU()]\n",
    "        for _ in range(num_mlps-1):\n",
    "            layers += [nn.Linear(final_hidden_size,\n",
    "                                 final_hidden_size), nn.ReLU()]\n",
    "        self.fcs = nn.Sequential(*layers)\n",
    "        self.fc_final = nn.Linear(final_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, sentence, aspect, pos_class, dep_tags, text_len, aspect_len, dep_rels, dep_heads, aspect_position, dep_dirs):\n",
    "        '''\n",
    "        Forward takes:\n",
    "            sentence: sentence_id of size (batch_size, text_length)\n",
    "            aspect: aspect_id of size (batch_size, aspect_length)\n",
    "            pos_class: pos_tag_id of size (batch_size, text_length)\n",
    "            dep_tags: dep_tag_id of size (batch_size, text_length)\n",
    "            text_len: (batch_size,) length of each sentence\n",
    "            aspect_len: (batch_size, ) aspect length of each sentence\n",
    "            dep_rels: (batch_size, text_length) relation\n",
    "            dep_heads: (batch_size, text_length) which node adjacent to that node\n",
    "            aspect_position: (batch_size, text_length) mask, with the position of aspect as 1 and others as 0\n",
    "            dep_dirs: (batch_size, text_length) the directions each node to the aspect\n",
    "        '''\n",
    "        fmask = (torch.zeros_like(sentence) != sentence).float()  # (Nï¼ŒL)\n",
    "        dmask = (torch.zeros_like(dep_tags) != dep_tags).float()  # (N ,L)\n",
    "\n",
    "        feature = self.embed(sentence)  # (N, L, D)\n",
    "        aspect_feature = self.embed(aspect) # (N, L', D)\n",
    "        feature = self.dropout(feature)\n",
    "        aspect_feature = self.dropout(aspect_feature)\n",
    "\n",
    "\n",
    "        if self.args.highway:\n",
    "            feature = self.highway(feature)\n",
    "            aspect_feature = self.highway(aspect_feature)\n",
    "\n",
    "        feature, _ = self.bilstm(feature) # (N,L,D)\n",
    "        aspect_feature, _ = self.bilstm(aspect_feature) #(N,L,D)\n",
    "\n",
    "        aspect_feature = aspect_feature.mean(dim = 1) # (N, D)\n",
    "\n",
    "        ############################################################################################\n",
    "\n",
    "        if self.args.gat_attention_type == 'gcn':\n",
    "            gat_out = self.gat(feature) # (N, L, D)\n",
    "            fmask = fmask.unsqueeze(2)\n",
    "            gat_out = gat_out * fmask\n",
    "            gat_out = F.relu(torch.sum(gat_out, dim = 1)) # (N, D)\n",
    "\n",
    "        else:\n",
    "            gat_out = [g(feature, aspect_feature, fmask).unsqueeze(1) for g in self.gat]\n",
    "            gat_out = torch.cat(gat_out, dim=1)\n",
    "            gat_out = gat_out.mean(dim=1)\n",
    "\n",
    "        feature_out = gat_out # (N, D')\n",
    "        # feature_out = gat_out\n",
    "        #############################################################################################\n",
    "        x = self.dropout(feature_out)\n",
    "        x = self.fcs(x)\n",
    "        logit = self.fc_final(x)\n",
    "        return logit\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:13:50.140756800Z",
     "start_time": "2023-05-09T05:13:50.116473700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# get dataset\n",
    "from load_dataset import Text2SQLDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_filepath = \"../data/resdsql_pre/preprocessed_dataset_test.json\"\n",
    "batch_size = 2 #'input batch size.')\n",
    "\n",
    "train_dataset = Text2SQLDataset(\n",
    "        dir_ = train_filepath,\n",
    "        mode = \"train\")\n",
    "\n",
    "train_dataloder = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        collate_fn = lambda x: x,\n",
    "        drop_last = True\n",
    "    )\n",
    "\n",
    "\n",
    "# def load_datasets_and_vocabs(args):\n",
    "#     train, test = get_dataset(args.dataset_name)\n",
    "#\n",
    "#     # Our model takes unrolled data, currently we don't consider the MAMS cases(future experiments)\n",
    "#     _, train_all_unrolled, _, _ = get_rolled_and_unrolled_data(train, args)\n",
    "#     _, test_all_unrolled, _, _ = get_rolled_and_unrolled_data(test, args)\n",
    "#\n",
    "#     logger.info('****** After unrolling ******')\n",
    "#     logger.info('Train set size: %s', len(train_all_unrolled))\n",
    "#     logger.info('Test set size: %s,', len(test_all_unrolled))\n",
    "#\n",
    "#     # Build word vocabulary(part of speech, dep_tag) and save pickles.\n",
    "#     word_vecs, word_vocab, dep_tag_vocab, pos_tag_vocab = load_and_cache_vocabs(\n",
    "#         train_all_unrolled+test_all_unrolled, args)\n",
    "#     if args.embedding_type == 'glove':\n",
    "#         embedding = torch.from_numpy(np.asarray(word_vecs, dtype=np.float32))\n",
    "#         args.glove_embedding = embedding\n",
    "#\n",
    "#     train_dataset = ASBA_Depparsed_Dataset(\n",
    "#         train_all_unrolled, args, word_vocab, dep_tag_vocab, pos_tag_vocab)\n",
    "#     test_dataset = ASBA_Depparsed_Dataset(\n",
    "#         test_all_unrolled, args, word_vocab, dep_tag_vocab, pos_tag_vocab)\n",
    "#\n",
    "#     return train_dataset, test_dataset, word_vocab, dep_tag_vocab, pos_tag_vocab\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:13:50.524343700Z",
     "start_time": "2023-05-09T05:13:50.138111900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_bert_optimizer(args, model):\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    # scheduler = WarmupLinearSchedule(\n",
    "    #     optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "    return optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "\n",
    "\n",
    "def train_2( model, num_epochs, train_dataloder, optimizer, scheduler ) :\n",
    "    # Train the model\n",
    "    # Load the data and tokenizer\n",
    "    # tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    text2sql_tokenizer = T5TokenizerFast.from_pretrained(\n",
    "        't5-small',\n",
    "        add_prefix_space = True\n",
    "    )\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx, batch in enumerate(train_dataloder):\n",
    "\n",
    "            batch_inputs = [data[0] for data in batch]\n",
    "            batch_sqls = [data[1] for data in batch]\n",
    "\n",
    "            if epoch == 0 and idx == 0:\n",
    "                for batch_id in range(len(batch_inputs)):\n",
    "                    print(f\"batch_inputs - {batch_inputs[batch_id]}\")\n",
    "                    print(f\"batch_sqls - {batch_sqls[batch_id]}\")\n",
    "    #                 print(\"----------------------\")\n",
    "\n",
    "            tokenized_inputs = text2sql_tokenizer(\n",
    "                batch_inputs,\n",
    "                padding = \"max_length\",\n",
    "                return_tensors = \"pt\",\n",
    "                max_length = 512, #512, max_encoder_len\n",
    "                truncation = True\n",
    "            )\n",
    "\n",
    "            with text2sql_tokenizer.as_target_tokenizer():\n",
    "                tokenized_outputs = text2sql_tokenizer(\n",
    "                    batch_sqls,\n",
    "                    padding = \"max_length\",\n",
    "                    return_tensors = 'pt',\n",
    "                    max_length = 256, #256, max_decoder_len\n",
    "                    truncation = True\n",
    "                )\n",
    "\n",
    "            encoder_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
    "            encoder_input_attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "            decoder_input_ids = tokenized_outputs[\"input_ids\"].to(device)\n",
    "            decoder_attention_mask = tokenized_outputs[\"attention_mask\"].to(device)\n",
    "            labels = None #tokenized_outputs[\"attention_mask\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(input_ids=encoder_input_ids,\n",
    "                         attention_mask=encoder_input_attention_mask,\n",
    "                         decoder_input_ids=decoder_input_ids,\n",
    "                         decoder_attention_mask=decoder_attention_mask,\n",
    "                         labels=labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            break\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'RGAT_model.pt')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:13:52.441006100Z",
     "start_time": "2023-05-09T05:13:52.385423200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "#\n",
    "# def train(args, train_dataset, model, test_dataset):\n",
    "#     '''Train the model'''\n",
    "#     tb_writer = SummaryWriter()\n",
    "#\n",
    "#     args.train_batch_size = args.per_gpu_train_batch_size\n",
    "#     train_sampler = RandomSampler(train_dataset)\n",
    "#\n",
    "#     train_dataloader = DataLoader(train_dataset, sampler=train_sampler,\n",
    "#                                   batch_size=args.train_batch_size)\n",
    "#\n",
    "#     if args.max_steps > 0:\n",
    "#         t_total = args.max_steps\n",
    "#         args.num_train_epochs = args.max_steps // (\n",
    "#             len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "#     else:\n",
    "#         t_total = len(\n",
    "#             train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "#\n",
    "#\n",
    "#     if args.embedding_type == 'bert':\n",
    "#         optimizer = get_bert_optimizer(args, model)\n",
    "#     else:\n",
    "#         parameters = filter(lambda param: param.requires_grad, model.parameters())\n",
    "#         optimizer = torch.optim.Adam(parameters, lr=args.learning_rate)\n",
    "#\n",
    "#     # Train\n",
    "#     logger.info(\"***** Running training *****\")\n",
    "#     logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "#     logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "#     logger.info(\"  Instantaneous batch size per GPU = %d\",\n",
    "#                 args.per_gpu_train_batch_size)\n",
    "#     logger.info(\"  Gradient Accumulation steps = %d\",\n",
    "#                 args.gradient_accumulation_steps)\n",
    "#     logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "#\n",
    "#\n",
    "#     global_step = 0\n",
    "#     tr_loss, logging_loss = 0.0, 0.0\n",
    "#     all_eval_results = []\n",
    "#     model.zero_grad()\n",
    "#     train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "#     set_seed(args)\n",
    "#     for _ in train_iterator:\n",
    "#         # epoch_iterator = tqdm(train_dataloader, desc='Iteration')\n",
    "#         for step, batch in enumerate(train_dataloader):\n",
    "#             model.train()\n",
    "#             batch = tuple(t.to(args.device) for t in batch)\n",
    "#\n",
    "#             inputs, labels = get_input_from_batch(args, batch)\n",
    "#             logit = model(**inputs)\n",
    "#             loss = F.cross_entropy(logit, labels)\n",
    "#\n",
    "#             if args.gradient_accumulation_steps > 1:\n",
    "#                 loss = loss / args.gradient_accumulation_steps\n",
    "#\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(\n",
    "#                 model.parameters(), args.max_grad_norm)\n",
    "#\n",
    "#             tr_loss += loss.item()\n",
    "#             if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "#                 # scheduler.step()  # Update learning rate schedule\n",
    "#                 optimizer.step()\n",
    "#                 model.zero_grad()\n",
    "#                 global_step += 1\n",
    "#\n",
    "#                 # Log metrics\n",
    "#                 if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "#                     results, eval_loss = evaluate(args, test_dataset, model)\n",
    "#                     all_eval_results.append(results)\n",
    "#                     for key, value in results.items():\n",
    "#                         tb_writer.add_scalar(\n",
    "#                             'eval_{}'.format(key), value, global_step)\n",
    "#                     tb_writer.add_scalar('eval_loss', eval_loss, global_step)\n",
    "#                     # tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "#                     tb_writer.add_scalar(\n",
    "#                         'train_loss', (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "#                     logging_loss = tr_loss\n",
    "#\n",
    "#                 # Save model checkpoint\n",
    "#\n",
    "#             if args.max_steps > 0 and global_step > args.max_steps:\n",
    "#                 # epoch_iterator.close()\n",
    "#                 break\n",
    "#         if args.max_steps > 0 and global_step > args.max_steps:\n",
    "#             # epoch_iterator.close()\n",
    "#             break\n",
    "#\n",
    "#     tb_writer.close()\n",
    "#     return global_step, tr_loss/global_step, all_eval_results\n",
    "#\n",
    "#\n",
    "#\n",
    "# # def evaluate(args, eval_dataset, model):\n",
    "# #     results = {}\n",
    "# #\n",
    "# #     args.eval_batch_size = args.per_gpu_eval_batch_size\n",
    "# #     eval_sampler = SequentialSampler(eval_dataset)\n",
    "# #     # collate_fn = get_collate_fn(args)\n",
    "# #     eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,\n",
    "# #                                  batch_size=args.eval_batch_size) #,\n",
    "# #                                  # collate_fn=collate_fn)\n",
    "# #\n",
    "# #     # Eval\n",
    "# #     logger.info(\"***** Running evaluation *****\")\n",
    "# #     logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "# #     logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "# #     eval_loss = 0.0\n",
    "# #     nb_eval_steps = 0\n",
    "# #     preds = None\n",
    "# #     out_label_ids = None\n",
    "# #     for batch in eval_dataloader:\n",
    "# #     # for batch in tqdm(eval_dataloader, desc='Evaluating'):\n",
    "# #         model.eval()\n",
    "# #         batch = tuple(t.to(args.device) for t in batch)\n",
    "# #         with torch.no_grad():\n",
    "# #             inputs, labels = get_input_from_batch(args, batch)\n",
    "# #\n",
    "# #             logits = model(**inputs)\n",
    "# #             tmp_eval_loss = F.cross_entropy(logits, labels)\n",
    "# #\n",
    "# #             eval_loss += tmp_eval_loss.mean().item()\n",
    "# #         nb_eval_steps += 1\n",
    "# #         if preds is None:\n",
    "# #             preds = logits.detach().cpu().numpy()\n",
    "# #             out_label_ids = labels.detach().cpu().numpy()\n",
    "# #         else:\n",
    "# #             preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "# #             out_label_ids = np.append(\n",
    "# #                 out_label_ids, labels.detach().cpu().numpy(), axis=0)\n",
    "# #\n",
    "# #     eval_loss = eval_loss / nb_eval_steps\n",
    "# #     preds = np.argmax(preds, axis=1)\n",
    "# #     # print(preds)\n",
    "# #     result = compute_metrics(preds, out_label_ids)\n",
    "# #     results.update(result)\n",
    "# #\n",
    "# #     output_eval_file = os.path.join(args.output_dir, 'eval_RGAT_results.txt')\n",
    "# #     with open(output_eval_file, 'a+') as writer:\n",
    "# #         logger.info('***** Eval results *****')\n",
    "# #         logger.info(\"  eval loss: %s\", str(eval_loss))\n",
    "# #         for key in sorted(result.keys()):\n",
    "# #             logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "# #             writer.write(\"  %s = %s\\n\" % (key, str(result[key])))\n",
    "# #             writer.write('\\n')\n",
    "# #         writer.write('\\n')\n",
    "# #     return results, eval_loss\n",
    "#\n",
    "#\n",
    "# def simple_accuracy(preds, labels):\n",
    "#     return (preds == labels).mean()\n",
    "#\n",
    "#\n",
    "# def acc_and_f1(preds, labels):\n",
    "#     acc = simple_accuracy(preds, labels)\n",
    "#     f1 = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "#     return {\n",
    "#         \"acc\": acc,\n",
    "#         \"f1\": f1\n",
    "#     }\n",
    "#\n",
    "# def compute_metrics(preds, labels):\n",
    "#     return acc_and_f1(preds, labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required parameters\n",
    "    # parser.add_argument('--dataset_name', type=str, default='rest',\n",
    "    #                     choices=['rest', 'laptop', 'twitter'],\n",
    "    #                     help='Choose absa dataset.')\n",
    "    # parser.add_argument('--output_dir', type=str, default='/data1/SHENWZH/ABSA_online/data/output-gcn',\n",
    "    #                     help='Directory to store intermedia data, such as vocab, embeddings, tags_vocab.')\n",
    "    # parser.add_argument('--num_classes', type=int, default=3,\n",
    "    #                     help='Number of classes of ABSA.')\n",
    "    #\n",
    "    #\n",
    "    # parser.add_argument('--cuda_id', type=str, default='3',\n",
    "    #                     help='Choose which GPUs to run')\n",
    "    # parser.add_argument('--seed', type=int, default=2019,\n",
    "    #                     help='random seed for initialization')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--glove_dir', type=str, default='/data1/SHENWZH/wordvec',\n",
    "                        help='Directory storing glove embeddings')\n",
    "    parser.add_argument('--bert_model_dir', type=str, default='/data1/SHENWZH/models/bert_base',\n",
    "                        help='Path to pre-trained Bert model.')\n",
    "    parser.add_argument('--pure_bert', action='store_true',\n",
    "                        help='Cat text and aspect, [cls] to predict.')\n",
    "    parser.add_argument('--gat_bert', action='store_true',\n",
    "                        help='Cat text and aspect, [cls] to predict.')\n",
    "\n",
    "    parser.add_argument('--highway', action='store_true',\n",
    "                        help='Use highway embed.')\n",
    "\n",
    "    parser.add_argument('--num_layers', type=int, default=2,\n",
    "                        help='Number of layers of bilstm or highway or elmo.')\n",
    "\n",
    "\n",
    "    parser.add_argument('--add_non_connect',  type= bool, default=True,\n",
    "                        help='Add a sepcial \"non-connect\" relation for aspect with no direct connection.')\n",
    "    parser.add_argument('--multi_hop',  type= bool, default=True,\n",
    "                        help='Multi hop non connection.')\n",
    "    parser.add_argument('--max_hop', type = int, default=4,\n",
    "                        help='max number of hops')\n",
    "\n",
    "\n",
    "    parser.add_argument('--num_heads', type=int, default=6,\n",
    "                        help='Number of heads for gat.')\n",
    "\n",
    "    parser.add_argument('--dropout', type=float, default=0,\n",
    "                        help='Dropout rate for embedding.')\n",
    "\n",
    "\n",
    "    parser.add_argument('--num_gcn_layers', type=int, default=1,\n",
    "                        help='Number of GCN layers.')\n",
    "    parser.add_argument('--gcn_mem_dim', type=int, default=300,\n",
    "                        help='Dimension of the W in GCN.')\n",
    "    parser.add_argument('--gcn_dropout', type=float, default=0.2,\n",
    "                        help='Dropout rate for GCN.')\n",
    "    # GAT\n",
    "    parser.add_argument('--gat', action='store_true',\n",
    "                        help='GAT')\n",
    "    parser.add_argument('--gat_our', action='store_true',\n",
    "                        help='GAT_our')\n",
    "    parser.add_argument('--gat_attention_type', type = str, choices=['linear','dotprod','gcn'], default='dotprod',\n",
    "                        help='The attention used for gat')\n",
    "\n",
    "    parser.add_argument('--embedding_type', type=str,default='glove', choices=['glove','bert'])\n",
    "    parser.add_argument('--embedding_dim', type=int, default=300,\n",
    "                        help='Dimension of glove embeddings')\n",
    "    parser.add_argument('--dep_relation_embed_dim', type=int, default=300,\n",
    "                        help='Dimension for dependency relation embeddings.')\n",
    "\n",
    "    parser.add_argument('--hidden_size', type=int, default=300,\n",
    "                        help='Hidden size of bilstm, in early stage.')\n",
    "    parser.add_argument('--final_hidden_size', type=int, default=300,\n",
    "                        help='Hidden size of bilstm, in early stage.')\n",
    "    parser.add_argument('--num_mlps', type=int, default=2,\n",
    "                        help='Number of mlps in the last of model.')\n",
    "\n",
    "    # Training parameters\n",
    "    parser.add_argument(\"--per_gpu_train_batch_size\", default=16, type=int,\n",
    "                        help=\"Batch size per GPU/CPU for training.\")\n",
    "    parser.add_argument(\"--per_gpu_eval_batch_size\", default=32, type=int,\n",
    "                        help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=2,\n",
    "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-3, type=float,\n",
    "                        help=\"The initial learning rate for Adam.\")\n",
    "\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
    "                        help=\"Weight deay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                        help=\"Epsilon for Adam optimizer.\")\n",
    "\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                        help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", default=30.0, type=float,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
    "                        help=\"If > 0: set total number of training steps(that update the weights) to perform. Override num_train_epochs.\")\n",
    "    parser.add_argument('--logging_steps', type=int, default=50,\n",
    "                        help=\"Log every X updates steps.\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def check_args(args):\n",
    "    '''\n",
    "    eliminate confilct situations\n",
    "\n",
    "    '''\n",
    "    logger.info(vars(args))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:21:24.682436900Z",
     "start_time": "2023-05-09T05:21:24.648582700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--glove_dir GLOVE_DIR]\n",
      "                             [--bert_model_dir BERT_MODEL_DIR] [--pure_bert]\n",
      "                             [--gat_bert] [--highway]\n",
      "                             [--num_layers NUM_LAYERS]\n",
      "                             [--add_non_connect ADD_NON_CONNECT]\n",
      "                             [--multi_hop MULTI_HOP] [--max_hop MAX_HOP]\n",
      "                             [--num_heads NUM_HEADS] [--dropout DROPOUT]\n",
      "                             [--num_gcn_layers NUM_GCN_LAYERS]\n",
      "                             [--gcn_mem_dim GCN_MEM_DIM]\n",
      "                             [--gcn_dropout GCN_DROPOUT] [--gat] [--gat_our]\n",
      "                             [--gat_attention_type {linear,dotprod,gcn}]\n",
      "                             [--embedding_type {glove,bert}]\n",
      "                             [--embedding_dim EMBEDDING_DIM]\n",
      "                             [--dep_relation_embed_dim DEP_RELATION_EMBED_DIM]\n",
      "                             [--hidden_size HIDDEN_SIZE]\n",
      "                             [--final_hidden_size FINAL_HIDDEN_SIZE]\n",
      "                             [--num_mlps NUM_MLPS]\n",
      "                             [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                             [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--logging_steps LOGGING_STEPS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Atharva\\AppData\\Roaming\\jupyter\\runtime\\kernel-734bffdc-adcb-4939-9559-9c3cf356e804.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Parse args\n",
    "# args = parse_args()\n",
    "# check_args(args)\n",
    "\n",
    "# Bert, load pretrained model and tokenizer, check if neccesary to put bert here\n",
    "if args.embedding_type == 'bert':\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model_dir)\n",
    "    args.tokenizer = tokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Load datasets and vocabs\n",
    "# train_dataset, test_dataset, word_vocab, dep_tag_vocab, pos_tag_vocab\n",
    "# Build Model\n",
    "model = Aspect_Text_GAT_only(args) #, dep_tag_vocab['len'], pos_tag_vocab['len'])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "model.to(device)\n",
    "# Train\n",
    "num_epochs = 10\n",
    "# Define the optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "\n",
    "# _, _,  all_eval_results = \\\n",
    "train_2( model, num_epochs, train_dataloder, optimizer, scheduler) # test_dataset\n",
    "\n",
    "# if len(all_eval_results):\n",
    "#     best_eval_result = max(all_eval_results, key=lambda x: x['acc'])\n",
    "#     for key in sorted(best_eval_result.keys()):\n",
    "#         logger.info(\"  %s = %s\", key, str(best_eval_result[key]))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:21:25.269528400Z",
     "start_time": "2023-05-09T05:21:25.227495Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T05:16:16.517866200Z",
     "start_time": "2023-05-09T05:16:16.472659900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
