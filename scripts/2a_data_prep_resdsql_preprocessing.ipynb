{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b271511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#\n",
    "# Reference: https: //github.com/RUCKBReasoning/RESDSQL.git\n",
    "# Reference script: preprocessing.py\n",
    "#\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4453a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from bridge_content_encoder import get_database_matches\n",
    "from sql_metadata import Parser\n",
    "from tqdm import tqdm\n",
    "\n",
    "sql_keywords = ['select', 'from', 'where', 'group', 'order', 'limit', 'intersect', 'union', \\\n",
    "    'except', 'join', 'on', 'as', 'not', 'between', 'in', 'like', 'is', 'exists', 'max', 'min', \\\n",
    "        'count', 'sum', 'avg', 'and', 'or', 'desc', 'asc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f5fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_contents(question, table_name_original, column_names_original, db_id, db_path):\n",
    "    matched_contents = []\n",
    "    # extract matched contents for each column\n",
    "    for column_name_original in column_names_original:\n",
    "        matches = get_database_matches(\n",
    "            question, \n",
    "            table_name_original, \n",
    "            column_name_original, \n",
    "            db_path + \"/{}/{}.sqlite\".format(db_id, db_id)\n",
    "        )\n",
    "        matches = sorted(matches)\n",
    "        matched_contents.append(matches)\n",
    "    \n",
    "    return matched_contents\n",
    "\n",
    "def get_db_schemas(all_db_infos):\n",
    "    db_schemas = {}\n",
    "\n",
    "    for db in all_db_infos:\n",
    "        table_names_original = db[\"table_names_original\"]\n",
    "        table_names = db[\"table_names\"]\n",
    "        column_names_original = db[\"column_names_original\"]\n",
    "        column_names = db[\"column_names\"]\n",
    "        column_types = db[\"column_types\"]\n",
    "\n",
    "        db_schemas[db[\"db_id\"]] = {}\n",
    "        \n",
    "        primary_keys, foreign_keys = [], []\n",
    "        # record primary keys\n",
    "        for pk_column_idx in db[\"primary_keys\"]:\n",
    "            pk_table_name_original = table_names_original[column_names_original[pk_column_idx][0]]\n",
    "            pk_column_name_original = column_names_original[pk_column_idx][1]\n",
    "            \n",
    "            primary_keys.append(\n",
    "                {\n",
    "                    \"table_name_original\": pk_table_name_original.lower(), \n",
    "                    \"column_name_original\": pk_column_name_original.lower()\n",
    "                }\n",
    "            )\n",
    "\n",
    "        db_schemas[db[\"db_id\"]][\"pk\"] = primary_keys\n",
    "\n",
    "        # record foreign keys\n",
    "        for source_column_idx, target_column_idx in db[\"foreign_keys\"]:\n",
    "            fk_source_table_name_original = table_names_original[column_names_original[source_column_idx][0]]\n",
    "            fk_source_column_name_original = column_names_original[source_column_idx][1]\n",
    "\n",
    "            fk_target_table_name_original = table_names_original[column_names_original[target_column_idx][0]]\n",
    "            fk_target_column_name_original = column_names_original[target_column_idx][1]\n",
    "            \n",
    "            foreign_keys.append(\n",
    "                {\n",
    "                    \"source_table_name_original\": fk_source_table_name_original.lower(),\n",
    "                    \"source_column_name_original\": fk_source_column_name_original.lower(),\n",
    "                    \"target_table_name_original\": fk_target_table_name_original.lower(),\n",
    "                    \"target_column_name_original\": fk_target_column_name_original.lower(),\n",
    "                }\n",
    "            )\n",
    "        db_schemas[db[\"db_id\"]][\"fk\"] = foreign_keys\n",
    "\n",
    "        db_schemas[db[\"db_id\"]][\"schema_items\"] = []\n",
    "        for idx, table_name_original in enumerate(table_names_original):\n",
    "            column_names_original_list = []\n",
    "            column_names_list = []\n",
    "            column_types_list = []\n",
    "            \n",
    "            for column_idx, (table_idx, column_name_original) in enumerate(column_names_original):\n",
    "                if idx == table_idx:\n",
    "                    column_names_original_list.append(column_name_original.lower())\n",
    "                    column_names_list.append(column_names[column_idx][1].lower())\n",
    "                    column_types_list.append(column_types[column_idx])\n",
    "            \n",
    "            db_schemas[db[\"db_id\"]][\"schema_items\"].append({\n",
    "                \"table_name_original\": table_name_original.lower(),\n",
    "                \"table_name\": table_names[idx].lower(), \n",
    "                \"column_names\": column_names_list, \n",
    "                \"column_names_original\": column_names_original_list,\n",
    "                \"column_types\": column_types_list\n",
    "            })\n",
    "\n",
    "    return db_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fd476cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(sql):\n",
    "    def white_space_fix(s):\n",
    "        parsed_s = Parser(s)\n",
    "        s = \" \".join([token.value for token in parsed_s.tokens])\n",
    "\n",
    "        return s\n",
    "\n",
    "    # convert everything except text between single quotation marks to lower case\n",
    "    def lower(s):\n",
    "        in_quotation = False\n",
    "        out_s = \"\"\n",
    "        for char in s:\n",
    "            if in_quotation:\n",
    "                out_s += char\n",
    "            else:\n",
    "                out_s += char.lower()\n",
    "            \n",
    "            if char == \"'\":\n",
    "                if in_quotation:\n",
    "                    in_quotation = False\n",
    "                else:\n",
    "                    in_quotation = True\n",
    "        \n",
    "        return out_s\n",
    "    \n",
    "    # remove \";\"\n",
    "    def remove_semicolon(s):\n",
    "        if s.endswith(\";\"):\n",
    "            s = s[:-1]\n",
    "        return s\n",
    "\n",
    "    # double quotation -> single quotation \n",
    "    def double2single(s):\n",
    "        return s.replace(\"\\\"\", \"'\") \n",
    "    \n",
    "    def add_asc(s):\n",
    "        pattern = re.compile(r'order by (?:\\w+ \\( \\S+ \\)|\\w+\\.\\w+|\\w+)(?: (?:\\+|\\-|\\<|\\<\\=|\\>|\\>\\=) (?:\\w+ \\( \\S+ \\)|\\w+\\.\\w+|\\w+))*')\n",
    "        if \"order by\" in s and \"asc\" not in s and \"desc\" not in s:\n",
    "            for p_str in pattern.findall(s):\n",
    "                s = s.replace(p_str, p_str + \" asc\")\n",
    "\n",
    "        return s\n",
    "\n",
    "    def remove_table_alias(s):\n",
    "        tables_aliases = Parser(s).tables_aliases\n",
    "        new_tables_aliases = {}\n",
    "        for i in range(1,11):\n",
    "            if \"t{}\".format(i) in tables_aliases.keys():\n",
    "                new_tables_aliases[\"t{}\".format(i)] = tables_aliases[\"t{}\".format(i)]\n",
    "        \n",
    "        tables_aliases = new_tables_aliases\n",
    "        for k, v in tables_aliases.items():\n",
    "            s = s.replace(\"as \" + k + \" \", \"\")\n",
    "            s = s.replace(k, v)\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    processing_func = lambda x : remove_table_alias(add_asc(lower(white_space_fix(double2single(remove_semicolon(x))))))\n",
    "    \n",
    "    return processing_func(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b70230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the skeleton of sql and natsql\n",
    "def extract_skeleton(sql, db_schema):\n",
    "    table_names_original, table_dot_column_names_original, column_names_original = [], [], []\n",
    "    for table in db_schema[\"schema_items\"]:\n",
    "        table_name_original = table[\"table_name_original\"]\n",
    "        table_names_original.append(table_name_original)\n",
    "\n",
    "        for column_name_original in [\"*\"]+table[\"column_names_original\"]:\n",
    "            table_dot_column_names_original.append(table_name_original+\".\"+column_name_original)\n",
    "            column_names_original.append(column_name_original)\n",
    "    \n",
    "    parsed_sql = Parser(sql)\n",
    "    new_sql_tokens = []\n",
    "    for token in parsed_sql.tokens:\n",
    "        # mask table names\n",
    "        if token.value in table_names_original:\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        # mask column names\n",
    "        elif token.value in column_names_original \\\n",
    "            or token.value in table_dot_column_names_original:\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        # mask string values\n",
    "        elif token.value.startswith(\"'\") and token.value.endswith(\"'\"):\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        # mask positive int number\n",
    "        elif token.value.isdigit():\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        # mask negative int number\n",
    "        elif isNegativeInt(token.value):\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        # mask float number\n",
    "        elif isFloat(token.value):\n",
    "            new_sql_tokens.append(\"_\")\n",
    "        else:\n",
    "            new_sql_tokens.append(token.value.strip())\n",
    "\n",
    "    sql_skeleton = \" \".join(new_sql_tokens)\n",
    "    \n",
    "    # remove JOIN ON keywords\n",
    "    sql_skeleton = sql_skeleton.replace(\"on _ = _ and _ = _\", \"on _ = _\")\n",
    "    sql_skeleton = sql_skeleton.replace(\"on _ = _ or _ = _\", \"on _ = _\")\n",
    "    sql_skeleton = sql_skeleton.replace(\" on _ = _\", \"\")\n",
    "    pattern3 = re.compile(\"_ (?:join _ ?)+\")\n",
    "    sql_skeleton = re.sub(pattern3, \"_ \", sql_skeleton)\n",
    "\n",
    "    # \"_ , _ , ..., _\" -> \"_\"\n",
    "    while(\"_ , _\" in sql_skeleton):\n",
    "        sql_skeleton = sql_skeleton.replace(\"_ , _\", \"_\")\n",
    "    \n",
    "    # remove clauses in WHERE keywords\n",
    "    ops = [\"=\", \"!=\", \">\", \">=\", \"<\", \"<=\"]\n",
    "    for op in ops:\n",
    "        if \"_ {} _\".format(op) in sql_skeleton:\n",
    "            sql_skeleton = sql_skeleton.replace(\"_ {} _\".format(op), \"_\")\n",
    "    while(\"where _ and _\" in sql_skeleton or \"where _ or _\" in sql_skeleton):\n",
    "        if \"where _ and _\"in sql_skeleton:\n",
    "            sql_skeleton = sql_skeleton.replace(\"where _ and _\", \"where _\")\n",
    "        if \"where _ or _\" in sql_skeleton:\n",
    "            sql_skeleton = sql_skeleton.replace(\"where _ or _\", \"where _\")\n",
    "\n",
    "    # remove additional spaces in the skeleton\n",
    "    while \"  \" in sql_skeleton:\n",
    "        sql_skeleton = sql_skeleton.replace(\"  \", \" \")\n",
    "\n",
    "    return sql_skeleton\n",
    "\n",
    "def isNegativeInt(string):\n",
    "    if string.startswith(\"-\") and string[1:].isdigit():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def isFloat(string):\n",
    "    if string.startswith(\"-\"):\n",
    "        string = string[1:]\n",
    "    \n",
    "    s = string.split(\".\")\n",
    "    if len(s)>2:\n",
    "        return False\n",
    "    else:\n",
    "        for s_i in s:\n",
    "            if not s_i.isdigit():\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245784d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk import word_tokenize\n",
    "\n",
    "max_encode_len = 0\n",
    "cnt = collections.Counter()\n",
    "\n",
    "VALUE_NUM_SYMBOL = \"{value}\"\n",
    "\n",
    "def get_encode_Query(query):\n",
    "    \n",
    "    global max_encode_len\n",
    "    global cnt\n",
    "    \n",
    "    tokens = strip_nl(query)\n",
    "    cnt.update(tokens)\n",
    "    max_encode_len = max(max_encode_len, len(tokens))\n",
    "    token_sentence = \" \".join(tokens)\n",
    "\n",
    "    return token_sentence\n",
    "\n",
    "def strip_nl(nl):\n",
    "    '''\n",
    "    return keywords of nl query\n",
    "    '''\n",
    "    nl_keywords = []\n",
    "    nl = nl.strip()\n",
    "    nl = nl.replace(\";\",\" ; \").replace(\",\", \" , \").replace(\"?\", \" ? \").replace(\"\\t\",\" \")\n",
    "    nl = nl.replace(\"(\", \" ( \").replace(\")\", \" ) \")\n",
    "    \n",
    "    nl = nl.replace('\\\"', \"'\")\n",
    "    nl = nl.replace(\"\\'\", \"'\")\n",
    "    \n",
    "#     str_1 = re.findall(\"\\\"[^\\\"]*\\\"\", nl)\n",
    "#     str_2 = re.findall(\"\\'[^\\']*\\'\", nl)\n",
    "#     float_nums = re.findall(\"[-+]?\\d*\\.\\d+\", nl)\n",
    "#     values = str_1 + str_2 + float_nums\n",
    "# #     print(values)\n",
    "#     for val in values:\n",
    "#         nl = nl.replace(val.strip(), VALUE_NUM_SYMBOL)\n",
    "\n",
    "    def to_lower(s):\n",
    "        in_quotation = False\n",
    "        out_s = \"\"\n",
    "        for char in s:\n",
    "            if in_quotation:\n",
    "                out_s += char\n",
    "            elif char == \".\":\n",
    "                if in_quotation:\n",
    "                    out_s += char\n",
    "                else:\n",
    "                    out_s += \" . \"\n",
    "            else:\n",
    "                out_s += char.lower()\n",
    "\n",
    "            if char == \"'\":\n",
    "                if in_quotation:\n",
    "                    in_quotation = False\n",
    "                else:\n",
    "                    in_quotation = True\n",
    "            \n",
    "        return out_s\n",
    "    \n",
    "    raw_keywords = nl.strip().split()\n",
    "#     print(raw_keywords)\n",
    "    for tok in raw_keywords:\n",
    "#         print(tok)\n",
    "        if \".\" in tok:\n",
    "            to = tok.split()\n",
    "#             to = tok.replace(\".\", \" . \").split()\n",
    "#             print(\"---\", to)\n",
    "#             to = [t.lower() for t in to if len(t)>0]\n",
    "            nl_keywords.extend(to)\n",
    "        elif \"'\" in tok and tok[0]!=\"'\" and tok[-1]!=\"'\":\n",
    "            to = word_tokenize(tok)\n",
    "#             to = [t.lower() for t in to if len(t)>0]\n",
    "            nl_keywords.extend(to)      \n",
    "#         elif len(tok) > 0:\n",
    "#             nl_keywords.append(tok.lower())\n",
    "        else:\n",
    "            nl_keywords.append(tok)\n",
    "    \n",
    "    nl_keywords = to_lower(\" \".join(nl_keywords))\n",
    "    nl_keywords_2 = nl_keywords.split(\" \")\n",
    "    \n",
    "    return nl_keywords_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c58584a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"find the last name of the latest contact individual of the enrico09@example . com organization 'Labour Party' . \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"Find the last name of the latest contact individual of the enrico09@example.com organization \\\"Labour Party\\\".\"\n",
    "output = get_encode_Query(sample)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dff9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83373a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_dataset_path,\n",
    "         output_dataset_path,\n",
    "         table_path,\n",
    "         db_path,\n",
    "         mode):\n",
    "    \n",
    "    max_len_decoder_target = 0\n",
    "    max_len_classifier_input = 0\n",
    "    \n",
    "    dataset = json.load(open(input_dataset_path))\n",
    "    print(f\"Total data points in {input_dataset_path.split('/')[-1]} is {len(dataset)}\")\n",
    "    all_db_infos = json.load(open(table_path))\n",
    "    \n",
    "    assert mode in [\"train\", \"eval\", \"test\"]\n",
    "    \n",
    "    db_schemas = get_db_schemas(all_db_infos)\n",
    "    \n",
    "    preprocessed_dataset = []\n",
    "\n",
    "#     for natsql_data, data in tqdm(zip(natsql_dataset, dataset)):\n",
    "    for data in tqdm(dataset):\n",
    "        if data['query'] == 'SELECT T1.company_name FROM Third_Party_Companies AS T1 JOIN Maintenance_Contracts AS T2 ON T1.company_id  =  T2.maintenance_contract_company_id JOIN Ref_Company_Types AS T3 ON T1.company_type_code  =  T3.company_type_code ORDER BY T2.contract_end_date DESC LIMIT 1':\n",
    "            data['query'] = 'SELECT T1.company_type FROM Third_Party_Companies AS T1 JOIN Maintenance_Contracts AS T2 ON T1.company_id  =  T2.maintenance_contract_company_id ORDER BY T2.contract_end_date DESC LIMIT 1'\n",
    "            data['query_toks'] = ['SELECT', 'T1.company_type', 'FROM', 'Third_Party_Companies', 'AS', 'T1', 'JOIN', 'Maintenance_Contracts', 'AS', 'T2', 'ON', 'T1.company_id', '=', 'T2.maintenance_contract_company_id', 'ORDER', 'BY', 'T2.contract_end_date', 'DESC', 'LIMIT', '1']\n",
    "            data['query_toks_no_value'] =  ['select', 't1', '.', 'company_type', 'from', 'third_party_companies', 'as', 't1', 'join', 'maintenance_contracts', 'as', 't2', 'on', 't1', '.', 'company_id', '=', 't2', '.', 'maintenance_contract_company_id', 'order', 'by', 't2', '.', 'contract_end_date', 'desc', 'limit', 'value']\n",
    "            data['question'] = 'What is the type of the company who concluded its contracts most recently?'\n",
    "            data['question_toks'] = ['What', 'is', 'the', 'type', 'of', 'the', 'company', 'who', 'concluded', 'its', 'contracts', 'most', 'recently', '?']\n",
    "        if data['query'].startswith('SELECT T1.fname FROM student AS T1 JOIN lives_in AS T2 ON T1.stuid  =  T2.stuid WHERE T2.dormid IN'):\n",
    "            data['query'] = data['query'].replace('IN (SELECT T2.dormid)', 'IN (SELECT T3.dormid)')\n",
    "            index = data['query_toks'].index('(') + 2\n",
    "            assert data['query_toks'][index] == 'T2.dormid'\n",
    "            data['query_toks'][index] = 'T3.dormid'\n",
    "            index = data['query_toks_no_value'].index('(') + 2\n",
    "            assert data['query_toks_no_value'][index] == 't2'\n",
    "            data['query_toks_no_value'][index] = 't3'\n",
    "\n",
    "        question = data[\"question\"].replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\").replace(\"\\u201c\", \"'\").replace(\"\\u201d\", \"'\").strip()\n",
    "        db_id = data[\"db_id\"]\n",
    "        \n",
    "        if mode == \"test\":\n",
    "            sql, norm_sql, sql_skeleton = \"\", \"\", \"\"\n",
    "            sql_tokens = []\n",
    "\n",
    "        else:\n",
    "            sql = data[\"query\"].strip()\n",
    "            norm_sql = normalization(sql).strip()\n",
    "            sql_skeleton = extract_skeleton(norm_sql, db_schemas[db_id]).strip()\n",
    "            sql_tokens = norm_sql.split()\n",
    "\n",
    "        norm_question = get_encode_Query(question)\n",
    "       \n",
    "        preprocessed_data = {}\n",
    "        preprocessed_data[\"question\"] = question\n",
    "        preprocessed_data[\"norm_question\"] = norm_question\n",
    "        preprocessed_data[\"db_id\"] = db_id\n",
    "        preprocessed_data[\"classifier_input\"] = question\n",
    "#         preprocessed_data[\"classifier_labels\"] = []\n",
    "\n",
    "        preprocessed_data[\"sql\"] = sql\n",
    "        preprocessed_data[\"norm_sql\"] = norm_sql\n",
    "        preprocessed_data[\"sql_skeleton\"] = sql_skeleton\n",
    "        preprocessed_data[\"decoder_target\"] = sql_skeleton + \" | \" + norm_sql\n",
    "        \n",
    "        preprocessed_data[\"db_schema\"] = []\n",
    "        preprocessed_data[\"pk\"] = db_schemas[db_id][\"pk\"]\n",
    "        preprocessed_data[\"fk\"] = db_schemas[db_id][\"fk\"]\n",
    "        preprocessed_data[\"table_labels\"] = []\n",
    "        preprocessed_data[\"column_labels\"] = []\n",
    "        \n",
    "        # add database information (including table name, column name, ..., table_labels, and column labels)\n",
    "        for table in db_schemas[db_id][\"schema_items\"]:\n",
    "            db_contents = get_db_contents(\n",
    "                question, \n",
    "                table[\"table_name_original\"], \n",
    "                table[\"column_names_original\"], \n",
    "                db_id, \n",
    "                db_path\n",
    "            )\n",
    "\n",
    "            preprocessed_data[\"db_schema\"].append({\n",
    "                \"table_name_original\":table[\"table_name_original\"],\n",
    "                \"table_name\":table[\"table_name\"],\n",
    "                \"column_names\":table[\"column_names\"],\n",
    "                \"column_names_original\":table[\"column_names_original\"],\n",
    "                \"column_types\":table[\"column_types\"],\n",
    "                \"db_contents\": db_contents\n",
    "            })\n",
    "\n",
    "            # extract table and column classification labels\n",
    "            if table[\"table_name_original\"] in sql_tokens:  # for used tables\n",
    "                preprocessed_data[\"table_labels\"].append(1)\n",
    "                column_labels = []\n",
    "                for column_name_original in table[\"column_names_original\"]:\n",
    "                    if column_name_original in sql_tokens or \\\n",
    "                        table[\"table_name_original\"]+\".\"+column_name_original in sql_tokens: # for used columns\n",
    "                        column_labels.append(1)\n",
    "                    else:\n",
    "                        column_labels.append(0)\n",
    "                preprocessed_data[\"column_labels\"].append(column_labels)\n",
    "            else:  # for unused tables and their columns\n",
    "                preprocessed_data[\"table_labels\"].append(0)\n",
    "                preprocessed_data[\"column_labels\"].append([0 for _ in range(len(table[\"column_names_original\"]))])\n",
    "                \n",
    "            # create classifier input\n",
    "            preprocessed_data[\"classifier_input\"] += \" | \" + table[\"table_name\"] + \": \"\n",
    "            \n",
    "            for idx_, col_names in enumerate(table[\"column_names_original\"]):\n",
    "                preprocessed_data[\"classifier_input\"] += col_names\n",
    "                \n",
    "                if idx_ < len(table[\"column_names_original\"])-1:\n",
    "                    preprocessed_data[\"classifier_input\"] += ', '\n",
    "                    \n",
    "        preprocessed_data[\"classifier_input\"] += \" | \" + sql_skeleton\n",
    "                    \n",
    "        # generate classifier labels\n",
    "#         for idx_, flag in enumerate(preprocessed_data[\"table_labels\"]):\n",
    "#             preprocessed_data[\"classifier_labels\"].append(flag)\n",
    "#             preprocessed_data[\"classifier_labels\"].append(flag)\n",
    "        \n",
    "#         print((preprocessed_data[\"classifier_input\"]).split(\" \"))\n",
    "        max_len_decoder_target = max(max_len_decoder_target,\n",
    "                                     len((preprocessed_data[\"decoder_target\"]).split(\" \")))\n",
    "#         max_len_classifier_input = max(max_len_classifier_input, len((preprocessed_data[\"classifier_input\"]).split(\" \")))\n",
    "        \n",
    "        preprocessed_dataset.append(preprocessed_data)\n",
    "#         break\n",
    "    \n",
    "    with open(output_dataset_path, \"w\") as f:\n",
    "        preprocessed_dataset_str = json.dumps(preprocessed_dataset, indent = 2)\n",
    "        f.write(preprocessed_dataset_str)\n",
    "        \n",
    "#     print(f\"Max encoder input length - {max_len_classifier_input}\")\n",
    "    print(f\"Max decoder input length - {max_len_decoder_target}\")\n",
    "        \n",
    "    print('Done')\n",
    "    \n",
    "    return preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3eb79043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points in dev.json is 1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1034/1034 [00:48<00:00, 21.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max decoder input length - 81\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider_data'\n",
    "table_path_1 = os.path.join(data_path, \"tables.json\")\n",
    "db_path_1 = os.path.join(data_path, \"database\")\n",
    "\n",
    "data_path_target = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/resdsql_pre\"\n",
    "if not os.path.isdir(data_path_target):\n",
    "    os.makedirs(data_path_target)\n",
    "    \n",
    "mode_1 = \"train\"\n",
    "input_dataset_path_1 = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_test.json\"\n",
    "input_dataset_path_1 = os.path.join(data_path, \"dev.json\") # \"train_spider.json\", dev.json    \n",
    "output_dataset_path_1 = os.path.join(data_path_target, \"preprocessed_dataset_dev.json\")\n",
    "\n",
    "processed_data = main(input_dataset_path_1,\n",
    "                         output_dataset_path_1,\n",
    "                         table_path_1,\n",
    "                         db_path_1,\n",
    "                         mode_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d9b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_data(input_dataset_path,\n",
    "                 output_dataset_path,\n",
    "                 table_path,\n",
    "                 db_path,\n",
    "                 mode):\n",
    "    \n",
    "    max_len_decoder_target = 0\n",
    "    max_len_classifier_input = 0\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    dataset = json.load(open(input_dataset_path))\n",
    "    print(f\"Total data points in {input_dataset_path.split('/')[-1]} is {len(dataset)}\")\n",
    "    all_db_infos = json.load(open(table_path))\n",
    "    \n",
    "    assert mode in [\"train\", \"eval\", \"test\"]\n",
    "    \n",
    "    db_schemas = get_db_schemas(all_db_infos)\n",
    "    \n",
    "    preprocessed_dataset = []\n",
    "\n",
    "#     for natsql_data, data in tqdm(zip(natsql_dataset, dataset)):\n",
    "    for data in tqdm(dataset):\n",
    "        if data['query'] == 'SELECT T1.company_name FROM Third_Party_Companies AS T1 JOIN Maintenance_Contracts AS T2 ON T1.company_id  =  T2.maintenance_contract_company_id JOIN Ref_Company_Types AS T3 ON T1.company_type_code  =  T3.company_type_code ORDER BY T2.contract_end_date DESC LIMIT 1':\n",
    "            data['query'] = 'SELECT T1.company_type FROM Third_Party_Companies AS T1 JOIN Maintenance_Contracts AS T2 ON T1.company_id  =  T2.maintenance_contract_company_id ORDER BY T2.contract_end_date DESC LIMIT 1'\n",
    "            data['query_toks'] = ['SELECT', 'T1.company_type', 'FROM', 'Third_Party_Companies', 'AS', 'T1', 'JOIN', 'Maintenance_Contracts', 'AS', 'T2', 'ON', 'T1.company_id', '=', 'T2.maintenance_contract_company_id', 'ORDER', 'BY', 'T2.contract_end_date', 'DESC', 'LIMIT', '1']\n",
    "            data['query_toks_no_value'] =  ['select', 't1', '.', 'company_type', 'from', 'third_party_companies', 'as', 't1', 'join', 'maintenance_contracts', 'as', 't2', 'on', 't1', '.', 'company_id', '=', 't2', '.', 'maintenance_contract_company_id', 'order', 'by', 't2', '.', 'contract_end_date', 'desc', 'limit', 'value']\n",
    "            data['question'] = 'What is the type of the company who concluded its contracts most recently?'\n",
    "            data['question_toks'] = ['What', 'is', 'the', 'type', 'of', 'the', 'company', 'who', 'concluded', 'its', 'contracts', 'most', 'recently', '?']\n",
    "        if data['query'].startswith('SELECT T1.fname FROM student AS T1 JOIN lives_in AS T2 ON T1.stuid  =  T2.stuid WHERE T2.dormid IN'):\n",
    "            data['query'] = data['query'].replace('IN (SELECT T2.dormid)', 'IN (SELECT T3.dormid)')\n",
    "            index = data['query_toks'].index('(') + 2\n",
    "            assert data['query_toks'][index] == 'T2.dormid'\n",
    "            data['query_toks'][index] = 'T3.dormid'\n",
    "            index = data['query_toks_no_value'].index('(') + 2\n",
    "            assert data['query_toks_no_value'][index] == 't2'\n",
    "            data['query_toks_no_value'][index] = 't3'\n",
    "\n",
    "        question = data[\"question\"].replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\").replace(\"\\u201c\", \"'\").replace(\"\\u201d\", \"'\").strip()\n",
    "        db_id = data[\"db_id\"]\n",
    "        \n",
    "        if mode == \"test\":\n",
    "            sql, norm_sql, sql_skeleton = \"\", \"\", \"\"\n",
    "            sql_tokens = []\n",
    "\n",
    "        else:\n",
    "            sql = data[\"query\"].strip()\n",
    "            norm_sql = normalization(sql).strip()\n",
    "            sql_skeleton = extract_skeleton(norm_sql, db_schemas[db_id]).strip()\n",
    "            sql_tokens = norm_sql.split()\n",
    "\n",
    "        norm_question = get_encode_Query(question)\n",
    "       \n",
    "        preprocessed_data = {}\n",
    "        preprocessed_data[\"question\"] = question\n",
    "        preprocessed_data[\"norm_question\"] = norm_question\n",
    "        preprocessed_data[\"db_id\"] = db_id\n",
    "#         preprocessed_data[\"classifier_input\"] = question\n",
    "#         preprocessed_data[\"classifier_labels\"] = []\n",
    "\n",
    "        preprocessed_data[\"sql\"] = sql\n",
    "        preprocessed_data[\"norm_sql\"] = norm_sql\n",
    "        preprocessed_data[\"sql_skeleton\"] = sql_skeleton\n",
    "        preprocessed_data[\"decoder_target\"] = sql_skeleton + \" | \" + norm_sql\n",
    "        \n",
    "        # add database information (including table name, column name, ..., table_labels, and column labels)\n",
    "        for table in db_schemas[db_id][\"schema_items\"]:\n",
    "\n",
    "            # create classifier input\n",
    "            preprocessed_data[\"classifier_input\"] += \" | \" + table[\"table_name\"] + \": \"\n",
    "            \n",
    "            for idx_, col_names in enumerate(table[\"column_names_original\"]):\n",
    "                preprocessed_data[\"classifier_input\"] += col_names\n",
    "                \n",
    "                if idx_ < len(table[\"column_names_original\"])-1:\n",
    "                    preprocessed_data[\"classifier_input\"] += ', '\n",
    "                    \n",
    "#         preprocessed_data[\"classifier_input\"] += \" | \" + sql_skeleton\n",
    "        \n",
    "#         max_len_classifier_input = max(max_len_classifier_input, len((preprocessed_data[\"classifier_input\"]).split(\" \")))\n",
    "        \n",
    "        preprocessed_dataset.append(preprocessed_data)\n",
    "        counter += 1\n",
    "        if counter > 1: break\n",
    "    \n",
    "    with open(output_dataset_path, \"w\") as f:\n",
    "        preprocessed_dataset_str = json.dumps(preprocessed_dataset, indent = 2)\n",
    "        f.write(preprocessed_dataset_str)\n",
    "        \n",
    "    print(f\"Max encoder input length - {max_len_classifier_input}\")\n",
    "#     print(f\"Max decoder input length - {max_len_decoder_target}\")\n",
    "        \n",
    "    print('Done')\n",
    "    \n",
    "    return preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf4e108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points in dev.json is 1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                              | 0/1034 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'classifier_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m input_dataset_path_1 \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# \"train_spider.json\", dev.json    \u001b[39;00m\n\u001b[1;32m     12\u001b[0m output_dataset_path_1 \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path_target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed_dataset_dev.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset_path_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                             \u001b[49m\u001b[43moutput_dataset_path_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtable_path_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdb_path_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmode_1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 70\u001b[0m, in \u001b[0;36mprompt_data\u001b[0;34m(input_dataset_path, output_dataset_path, table_path, db_path, mode)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# add database information (including table name, column name, ..., table_labels, and column labels)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m db_schemas[db_id][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema_items\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# create classifier input\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[43mpreprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassifier_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx_, col_names \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(table[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn_names_original\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m     73\u001b[0m         preprocessed_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier_input\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m col_names\n",
      "\u001b[0;31mKeyError\u001b[0m: 'classifier_input'"
     ]
    }
   ],
   "source": [
    "data_path = '/Users/aishwarya/Downloads/spring23/cs685-NLP/project/spider_data'\n",
    "table_path_1 = os.path.join(data_path, \"tables.json\")\n",
    "db_path_1 = os.path.join(data_path, \"database\")\n",
    "\n",
    "data_path_target = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/resdsql_pre\"\n",
    "if not os.path.isdir(data_path_target):\n",
    "    os.makedirs(data_path_target)\n",
    "    \n",
    "mode_1 = \"train\"\n",
    "# input_dataset_path_1 = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/split/spider_test.json\"\n",
    "input_dataset_path_1 = os.path.join(data_path, \"dev.json\") # \"train_spider.json\", dev.json    \n",
    "output_dataset_path_1 = os.path.join(data_path_target, \"preprocessed_dataset_dev.json\")\n",
    "\n",
    "processed_data = prompt_data(input_dataset_path_1,\n",
    "                             output_dataset_path_1,\n",
    "                             table_path_1,\n",
    "                             db_path_1,\n",
    "                             mode_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e8dcf",
   "metadata": {},
   "source": [
    "## Check processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e21e0194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question - Find the payment method and phone of the party with email \"enrico09@example.com\".\n",
      "norm_question - find the payment method and phone of the party with email 'enrico09@example.com' . \n",
      "sql - SELECT payment_method_code ,  party_phone FROM parties WHERE party_email  =  \"enrico09@example.com\"\n",
      "norm_sql - select payment_method_code , party_phone from parties where party_email = 'enrico09@example.com'\n",
      "\n",
      "\n",
      "question - Find the last name of the latest contact individual of the organization \"Labour Party\".\n",
      "norm_question - find the last name of the latest contact individual of the organization 'Labour Party' . \n",
      "sql - SELECT t3.individual_last_name FROM organizations AS t1 JOIN organization_contact_individuals AS t2 ON t1.organization_id  =  t2.organization_id JOIN individuals AS t3 ON t2.individual_id  =  t3.individual_id WHERE t1.organization_name  =  \"Labour Party\" ORDER BY t2.date_contact_to DESC LIMIT 1\n",
      "norm_sql - select individuals.individual_last_name from organizations join organization_contact_individuals on organizations.organization_id = organization_contact_individuals.organization_id join individuals on organization_contact_individuals.individual_id = individuals.individual_id where organizations.organization_name = 'Labour Party' order by organization_contact_individuals.date_contact_to desc limit 1\n",
      "\n",
      "\n",
      "question - How many cities are there in state \"Colorado\"?\n",
      "norm_question - how many cities are there in state 'Colorado' ?\n",
      "sql - SELECT count(*) FROM addresses WHERE state_province_county  =  \"Colorado\"\n",
      "norm_sql - select count ( * ) from addresses where state_province_county = 'Colorado'\n",
      "\n",
      "\n",
      "question - Find the name of organizations whose names contain \"Party\".\n",
      "norm_question - find the name of organizations whose names contain 'Party' . \n",
      "sql - SELECT organization_name FROM organizations WHERE organization_name LIKE \"%Party%\"\n",
      "norm_sql - select organization_name from organizations where organization_name like '%Party%'\n",
      "\n",
      "\n",
      "question - What are the names of organizations that contain the word \"Party\"?\n",
      "norm_question - what are the names of organizations that contain the word 'Party' ?\n",
      "sql - SELECT organization_name FROM organizations WHERE organization_name LIKE \"%Party%\"\n",
      "norm_sql - select organization_name from organizations where organization_name like '%Party%'\n",
      "\n",
      "\n",
      "question - Which state can address \"6862 Kaitlyn Knolls\" possibly be in?\n",
      "norm_question - which state can address '6862 Kaitlyn Knolls' possibly be in ?\n",
      "sql - SELECT state_province_county FROM addresses WHERE line_1_number_building LIKE \"%6862 Kaitlyn Knolls%\"\n",
      "norm_sql - select state_province_county from addresses where line_1_number_building like '%6862 Kaitlyn Knolls%'\n",
      "\n",
      "\n",
      "question - Give the state corresponding to the line number building \"6862 Kaitlyn Knolls\".\n",
      "norm_question - give the state corresponding to the line number building '6862 Kaitlyn Knolls' . \n",
      "sql - SELECT state_province_county FROM addresses WHERE line_1_number_building LIKE \"%6862 Kaitlyn Knolls%\"\n",
      "norm_sql - select state_province_county from addresses where line_1_number_building like '%6862 Kaitlyn Knolls%'\n",
      "\n",
      "\n",
      "question - What is the project detail for the project with document \"King Book\"?\n",
      "norm_question - what is the project detail for the project with document 'King Book' ?\n",
      "sql - SELECT T1.project_details FROM Projects AS T1 JOIN Documents AS T2 ON T1.project_id  =  T2.project_id WHERE T2.document_name  =  \"King Book\"\n",
      "norm_sql - select projects.project_details from projects join documents on projects.project_id = documents.project_id where documents.document_name = 'King Book'\n",
      "\n",
      "\n",
      "question - What are the names of the stations which serve both \"Ananthapuri Express\" and \"Guruvayur Express\" trains?\n",
      "norm_question - what are the names of the stations which serve both 'Ananthapuri Express' and 'Guruvayur Express' trains ?\n",
      "sql - SELECT T2.name FROM train_station AS T1 JOIN station AS T2 ON T1.station_id  =  T2.station_id JOIN train AS T3 ON T3.train_id  =  T1.train_id WHERE T3.Name  =  \"Ananthapuri Express\" INTERSECT SELECT T2.name FROM train_station AS T1 JOIN station AS T2 ON T1.station_id  =  T2.station_id JOIN train AS T3 ON T3.train_id  =  T1.train_id WHERE T3.Name  =  \"Guruvayur Express\"\n",
      "norm_sql - select station.name from train_station join station on train_station.station_id = station.station_id join train on train.train_id = train_station.train_id where train.name = 'Ananthapuri Express' intersect select station.name from train_station join station on train_station.station_id = station.station_id join train on train.train_id = train_station.train_id where train.name = 'Guruvayur Express'\n",
      "\n",
      "\n",
      "question - Find the id of the order whose shipment tracking number is \"3452\".\n",
      "norm_question - find the id of the order whose shipment tracking number is '3452' . \n",
      "sql - SELECT order_id FROM shipments WHERE shipment_tracking_number = \"3452\"\n",
      "norm_sql - select order_id from shipments where shipment_tracking_number = '3452'\n",
      "\n",
      "\n",
      "question - Which order's shipment tracking number is \"3452\"? Give me the id of the order.\n",
      "norm_question - which order 's shipment tracking number is '3452' ? Give me the id of the order.\n",
      "sql - SELECT order_id FROM shipments WHERE shipment_tracking_number = \"3452\"\n",
      "norm_sql - select order_id from shipments where shipment_tracking_number = '3452'\n",
      "\n",
      "\n",
      "question - List the name of all the distinct customers who have orders with status \"Packing\".\n",
      "norm_question - list the name of all the distinct customers who have orders with status 'Packing' . \n",
      "sql - SELECT DISTINCT T1.customer_name FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"Packing\"\n",
      "norm_sql - select distinct customers.customer_name from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'Packing'\n",
      "\n",
      "\n",
      "question - Which customers have orders with status \"Packing\"? Give me the customer names.\n",
      "norm_question - which customers have orders with status 'Packing' ? give me the customer names . \n",
      "sql - SELECT DISTINCT T1.customer_name FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"Packing\"\n",
      "norm_sql - select distinct customers.customer_name from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'Packing'\n",
      "\n",
      "\n",
      "question - Find the details of all the distinct customers who have orders with status \"On Road\".\n",
      "norm_question - find the details of all the distinct customers who have orders with status 'On Road' . \n",
      "sql - SELECT DISTINCT T1.customer_details FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"On Road\"\n",
      "norm_sql - select distinct customers.customer_details from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'On Road'\n",
      "\n",
      "\n",
      "question - What are the distinct customers who have orders with status \"On Road\"? Give me the customer details?\n",
      "norm_question - what are the distinct customers who have orders with status 'On Road' ? give me the customer details ?\n",
      "sql - SELECT DISTINCT T1.customer_details FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"On Road\"\n",
      "norm_sql - select distinct customers.customer_details from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'On Road'\n",
      "\n",
      "\n",
      "question - Give me a list of id and status of orders which belong to the customer named \"Jeramie\".\n",
      "norm_question - give me a list of id and status of orders which belong to the customer named 'Jeramie' . \n",
      "sql - SELECT T2.order_id ,  T2.order_status FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T1.customer_name = \"Jeramie\"\n",
      "norm_sql - select orders.order_id , orders.order_status from customers join orders on customers.customer_id = orders.customer_id where customers.customer_name = 'Jeramie'\n",
      "\n",
      "\n",
      "question - Which orders are made by the customer named \"Jeramie\"? Give me the order ids and status.\n",
      "norm_question - which orders are made by the customer named 'Jeramie' ? give me the order ids and status . \n",
      "sql - SELECT T2.order_id ,  T2.order_status FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T1.customer_name = \"Jeramie\"\n",
      "norm_sql - select orders.order_id , orders.order_status from customers join orders on customers.customer_id = orders.customer_id where customers.customer_name = 'Jeramie'\n",
      "\n",
      "\n",
      "question - Find the dates of orders which belong to the customer named \"Jeramie\".\n",
      "norm_question - find the dates of orders which belong to the customer named 'Jeramie' . \n",
      "sql - SELECT T2.date_order_placed FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T1.customer_name = \"Jeramie\"\n",
      "norm_sql - select orders.date_order_placed from customers join orders on customers.customer_id = orders.customer_id where customers.customer_name = 'Jeramie'\n",
      "\n",
      "\n",
      "question - What are the dates of the orders made by the customer named \"Jeramie\"?\n",
      "norm_question - what are the dates of the orders made by the customer named 'Jeramie' ?\n",
      "sql - SELECT T2.date_order_placed FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T1.customer_name = \"Jeramie\"\n",
      "norm_sql - select orders.date_order_placed from customers join orders on customers.customer_id = orders.customer_id where customers.customer_name = 'Jeramie'\n",
      "\n",
      "\n",
      "question - Find the names of the customers who have order status both \"On Road\" and \"Shipped\".\n",
      "norm_question - find the names of the customers who have order status both 'On Road' and 'Shipped' . \n",
      "sql - SELECT T1.customer_name FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"On Road\" INTERSECT SELECT T1.customer_name FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"Shipped\"\n",
      "norm_sql - select customers.customer_name from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'On Road' intersect select customers.customer_name from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'Shipped'\n",
      "\n",
      "\n",
      "question - Which customers have both \"On Road\" and \"Shipped\" as order status? List the customer names.\n",
      "norm_question - which customers have both 'On Road' and 'Shipped' as order status ? list the customer names . \n",
      "sql - SELECT T1.customer_name FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"On Road\" INTERSECT SELECT T1.customer_name FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"Shipped\"\n",
      "norm_sql - select customers.customer_name from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'On Road' intersect select customers.customer_name from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'Shipped'\n",
      "\n",
      "\n",
      "question - Find the id of the customers who have order status both \"On Road\" and \"Shipped\".\n",
      "norm_question - find the id of the customers who have order status both 'On Road' and 'Shipped' . \n",
      "sql - SELECT T1.customer_id FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"On Road\" INTERSECT SELECT T1.customer_id FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"Shipped\"\n",
      "norm_sql - select customers.customer_id from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'On Road' intersect select customers.customer_id from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'Shipped'\n",
      "\n",
      "\n",
      "question - Which customers have both \"On Road\" and \"Shipped\" as order status? List the customer ids.\n",
      "norm_question - which customers have both 'On Road' and 'Shipped' as order status ? list the customer ids . \n",
      "sql - SELECT T1.customer_id FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"On Road\" INTERSECT SELECT T1.customer_id FROM customers AS T1 JOIN orders AS T2 ON T1.customer_id = T2.customer_id WHERE T2.order_status = \"Shipped\"\n",
      "norm_sql - select customers.customer_id from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'On Road' intersect select customers.customer_id from customers join orders on customers.customer_id = orders.customer_id where orders.order_status = 'Shipped'\n",
      "\n",
      "\n",
      "question - List the names of the customers who have once bought product \"food\".\n",
      "norm_question - list the names of the customers who have once bought product 'food' . \n",
      "sql - SELECT T1.customer_name FROM customers AS T1 JOIN orders AS T2 JOIN order_items AS T3 JOIN products AS T4 ON T1.customer_id = T2.customer_id AND T2.order_id = T3.order_id AND T3.product_id = T4.product_id WHERE T4.product_name = \"food\" GROUP BY T1.customer_id HAVING count(*)  >=  1\n",
      "norm_sql - select customers.customer_name from customers join orders join order_items join products on customers.customer_id = orders.customer_id and orders.order_id = order_items.order_id and order_items.product_id = products.product_id where products.product_name = 'food' group by customers.customer_id having count ( * ) >= 1\n",
      "\n",
      "\n",
      "question - What are the names of the customers who bought product \"food\" at least once?\n",
      "norm_question - what are the names of the customers who bought product 'food' at least once ?\n",
      "sql - SELECT T1.customer_name FROM customers AS T1 JOIN orders AS T2 JOIN order_items AS T3 JOIN products AS T4 ON T1.customer_id = T2.customer_id AND T2.order_id = T3.order_id AND T3.product_id = T4.product_id WHERE T4.product_name = \"food\" GROUP BY T1.customer_id HAVING count(*)  >=  1\n",
      "norm_sql - select customers.customer_name from customers join orders join order_items join products on customers.customer_id = orders.customer_id and orders.order_id = order_items.order_id and order_items.product_id = products.product_id where products.product_name = 'food' group by customers.customer_id having count ( * ) >= 1\n",
      "\n",
      "\n",
      "question - List the names of customers who have once canceled the purchase of the product \"food\" (the item status is \"Cancel\").\n",
      "norm_question - list the names of customers who have once canceled the purchase of the product 'food' ( the item status is 'Cancel' )  . \n",
      "sql - SELECT T1.customer_name FROM customers AS T1 JOIN orders AS T2 JOIN order_items AS T3 JOIN products AS T4 ON T1.customer_id = T2.customer_id AND T2.order_id = T3.order_id AND T3.product_id = T4.product_id WHERE T3.order_item_status = \"Cancel\" AND T4.product_name = \"food\" GROUP BY T1.customer_id HAVING count(*)  >=  1\n",
      "norm_sql - select customers.customer_name from customers join orders join order_items join products on customers.customer_id = orders.customer_id and orders.order_id = order_items.order_id and order_items.product_id = products.product_id where order_items.order_item_status = 'Cancel' and products.product_name = 'food' group by customers.customer_id having count ( * ) >= 1\n",
      "\n",
      "\n",
      "question - Which customers have ever canceled the purchase of the product \"food\" (the item status is \"Cancel\")?\n",
      "norm_question - which customers have ever canceled the purchase of the product 'food' ( the item status is 'Cancel' ) ?\n",
      "sql - SELECT T1.customer_name FROM customers AS T1 JOIN orders AS T2 JOIN order_items AS T3 JOIN products AS T4 ON T1.customer_id = T2.customer_id AND T2.order_id = T3.order_id AND T3.product_id = T4.product_id WHERE T3.order_item_status = \"Cancel\" AND T4.product_name = \"food\" GROUP BY T1.customer_id HAVING count(*)  >=  1\n",
      "norm_sql - select customers.customer_name from customers join orders join order_items join products on customers.customer_id = orders.customer_id and orders.order_id = order_items.order_id and order_items.product_id = products.product_id where order_items.order_item_status = 'Cancel' and products.product_name = 'food' group by customers.customer_id having count ( * ) >= 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for block in processed_data:\n",
    "    if \"\\\"\" in block[\"question\"]:\n",
    "        for k,v in block.items():\n",
    "            if k in [\"question\", \"norm_question\", \"sql\", \"norm_sql\"]:\n",
    "                print(f\"{k} - {v}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614b96d",
   "metadata": {},
   "source": [
    "## Check if classifier data loader works on above dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56933687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_dataset import ColumnAndTableClassifierDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a57fc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ColumnAndTableClassifierDataset(\n",
    "        dir_ = \"/Users/aishwarya/Downloads/spring23/cs685-NLP/project/data/resdsql_pre/preprocessed_dataset.json\",\n",
    "        use_contents = True,\n",
    "        add_fk_info = False\n",
    "    )\n",
    "\n",
    "train_dataloder = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = 1, \n",
    "        shuffle = True,\n",
    "        collate_fn = lambda x: x\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25719161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How many singers do we have?', ['stadium', 'singer', 'concert', 'singer in concert'], [0, 1, 0, 0], [['stadium id', 'location', 'name', 'capacity', 'highest', 'lowest', 'average'], ['singer id', 'name', 'country', 'song name', 'song release year', 'age', 'is male'], ['concert id', 'concert name', 'theme', 'stadium id', 'year'], ['concert id', 'singer id']], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloder:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cf88323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How many singers do we have?', ['stadium', 'singer', 'concert', 'singer in concert'], [0, 1, 0, 0], [['stadium id', 'location', 'name', 'capacity', 'highest', 'lowest', 'average'], ['singer id', 'name', 'country', 'song name', 'song release year', 'age', 'is male'], ['concert id', 'concert name', 'theme', 'stadium id', 'year'], ['concert id', 'singer id']], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloder:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "239ee7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_questions = [data[0] for data in batch]\n",
    "    \n",
    "batch_table_names = [data[1] for data in batch]\n",
    "batch_table_labels = [data[2] for data in batch]\n",
    "\n",
    "batch_column_infos = [data[3] for data in batch]\n",
    "batch_column_labels = [data[4] for data in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46c6e094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How many singers do we have?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "238f4644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['stadium', 'singer', 'concert', 'singer in concert']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "746847e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_table_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a65a3891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['stadium id',\n",
       "   'location',\n",
       "   'name',\n",
       "   'capacity',\n",
       "   'highest',\n",
       "   'lowest',\n",
       "   'average'],\n",
       "  ['singer id',\n",
       "   'name',\n",
       "   'country',\n",
       "   'song name',\n",
       "   'song release year',\n",
       "   'age',\n",
       "   'is male'],\n",
       "  ['concert id', 'concert name', 'theme', 'stadium id', 'year'],\n",
       "  ['concert id', 'singer id']]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_column_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0fa7235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_table_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bbd06a",
   "metadata": {},
   "source": [
    "## Check changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9926d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_nl(nl):\n",
    "    '''\n",
    "    return keywords of nl query\n",
    "    '''\n",
    "    nl_keywords = []\n",
    "    nl = nl.strip()\n",
    "    nl = nl.replace(\";\",\" ; \").replace(\",\", \" , \").replace(\"?\", \" ? \").replace(\"\\t\",\" \")\n",
    "    nl = nl.replace(\"(\", \" ( \").replace(\")\", \" ) \")\n",
    "    \n",
    "    nl = nl.replace('\\\"', \"'\")\n",
    "    nl = nl.replace(\"\\'\", \"'\")\n",
    "    \n",
    "#     str_1 = re.findall(\"\\\"[^\\\"]*\\\"\", nl)\n",
    "#     str_2 = re.findall(\"\\'[^\\']*\\'\", nl)\n",
    "#     float_nums = re.findall(\"[-+]?\\d*\\.\\d+\", nl)\n",
    "#     values = str_1 + str_2 + float_nums\n",
    "# #     print(values)\n",
    "#     for val in values:\n",
    "#         nl = nl.replace(val.strip(), VALUE_NUM_SYMBOL)\n",
    "\n",
    "    def to_lower(s):\n",
    "        in_quotation = False\n",
    "        out_s = \"\"\n",
    "        for char in s:\n",
    "            if in_quotation:\n",
    "                out_s += char\n",
    "            elif char == \".\":\n",
    "                if in_quotation:\n",
    "                    out_s += char\n",
    "                else:\n",
    "                    out_s += \" . \"\n",
    "            else:\n",
    "                out_s += char.lower()\n",
    "\n",
    "            if char == \"'\":\n",
    "                if in_quotation:\n",
    "                    in_quotation = False\n",
    "                else:\n",
    "                    in_quotation = True\n",
    "            \n",
    "        return out_s\n",
    "    \n",
    "    raw_keywords = nl.strip().split()\n",
    "#     print(raw_keywords)\n",
    "    for tok in raw_keywords:\n",
    "#         print(tok)\n",
    "        if \".\" in tok:\n",
    "            to = tok.split()\n",
    "#             to = tok.replace(\".\", \" . \").split()\n",
    "#             print(\"---\", to)\n",
    "#             to = [t.lower() for t in to if len(t)>0]\n",
    "            nl_keywords.extend(to)\n",
    "        elif \"'\" in tok and tok[0]!=\"'\" and tok[-1]!=\"'\":\n",
    "            to = word_tokenize(tok)\n",
    "#             to = [t.lower() for t in to if len(t)>0]\n",
    "            nl_keywords.extend(to)      \n",
    "#         elif len(tok) > 0:\n",
    "#             nl_keywords.append(tok.lower())\n",
    "        else:\n",
    "            nl_keywords.append(tok)\n",
    "    \n",
    "    nl_keywords = to_lower(\" \".join(nl_keywords))\n",
    "    nl_keywords_2 = nl_keywords.split(\" \")\n",
    "    \n",
    "    return nl_keywords_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843ab43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3be9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
